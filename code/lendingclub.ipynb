{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ae1b09e",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "Before even running the following script, please follow the first steps:\n",
    "\n",
    "- [ ] Installing the necessary librairies. You can comment the next script to avoid biding your time.\n",
    "\n",
    "- [ ] Replacing dataset path toward your correct local repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a923e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scipy\n",
    "!pip install tqdm\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install pyspark\n",
    "!pip install catboost\n",
    "!pip install shap\n",
    "!pip install pingouin\n",
    "!pip install seaborn\n",
    "!pip install torch\n",
    "!pip install captum\n",
    "!pip install sklearn\n",
    "!pip install mapie\n",
    "!pip install scikit-optimize\n",
    "!pip install pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b006539",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"/media/yanncauchepin/ExternalDisk/Datasets/MachineLearningTables/lending_club/LCDataDictionary.xlsx\"\n",
    "data_path = \"/media/yanncauchepin/ExternalDisk/Datasets/MachineLearningTables/lending_club/Loan_status_2007-2020Q3.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6939e91c",
   "metadata": {},
   "source": [
    "# Data Importation\n",
    "\n",
    "I initially plan to use **Pandas** librairy as I am used to work on few data, either in my current job or in my hands-on exercices. Since my computer freeze over many times, I decided to use **[Spark](https://spark.apache.org/)** library to manipulate this significative amount of data. This library involve map-reduce method to handle big data more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c14c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957f17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_excel(metadata_path, index_col=0)\n",
    "metadata = metadata.iloc[:-2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ddf746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LendingClubDataProcessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_spark = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "print(f\"Number of data: {df_spark.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1673bd",
   "metadata": {},
   "source": [
    "# Data Cleaning - Encoding Target\n",
    "\n",
    "Since the dataset was filled with metadata, I decided to check the consistency of the data to remove outer features. Additionnaly, I follow the instruction of removing *grade* and *sub_grade* features.\n",
    "\n",
    "Since I understood that the machine learning must classify whether a loan is *Fully Paid* or *Charged Off* specifically, I decided to encode the rest to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4751cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Metadata features: {len(metadata.index)}\")\n",
    "print(f\"Data features: {len(df_spark.columns)}\")\n",
    "\n",
    "outer_features = [feature for feature in df_spark.columns if feature not in metadata.index]\n",
    "\n",
    "print(f\"Unknown data features: {outer_features} ({len(outer_features)})\")\n",
    "df_spark = df_spark.drop(*outer_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8175f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = ['grade', 'sub_grade']\n",
    "df_spark = df_spark.drop(*features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3876cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df_spark.groupBy('loan_status').count()\n",
    "value_rate = value_counts.count() / df_spark.count()\n",
    "print(f\"Number of distincts values: {value_counts.count()} - {value_rate:.2e} %\")\n",
    "value_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mapping = {\n",
    "    'Fully Paid': 0,\n",
    "    'Charged Off': 1,\n",
    "    'Current': np.nan,\n",
    "    'Late (31-120 days)': np.nan,\n",
    "    'In Grace Period': np.nan,\n",
    "    'Late (16-30 days)': np.nan,\n",
    "    'Issued': np.nan,\n",
    "    'Does not meet the credit policy. Status:Fully Paid': np.nan,\n",
    "    'Does not meet the credit policy. Status:Charged Off': np.nan,\n",
    "    'Default': np.nan,\n",
    "    'Oct-2015': np.nan\n",
    "}\n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "df_spark = df_spark.withColumn(\"loan_status\", when(df_spark[\"loan_status\"] == \"Fully Paid\", 0)\n",
    "                   .when(df_spark[\"loan_status\"] == \"Charged Off\", 1)\n",
    "                   .otherwise(np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a37f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = [feature for feature in df_spark.columns if feature != 'loan_status']\n",
    "df_spark = df_spark.fillna({feature: \"nan\" if df_spark.schema[feature].dataType == 'string' else np.nan for feature in all_features})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205dda4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df_spark.groupBy('loan_status').count()\n",
    "value_rate = value_counts.count() / df_spark.count()\n",
    "print(f\"Number of distincts values: {value_counts.count()} - {value_rate:.2e} %\")\n",
    "value_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83528a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ced9754",
   "metadata": {},
   "source": [
    "# Data Reduction\n",
    "\n",
    "With the lack of time, and the complex structure of the table data, mixing types and nan, an advanced analysis would be interesting to pursue a modeling process. However, there is a interesting preprocess that consist to reduce the mix data into a low dimensional relevant data. In fact, this is common in large matrix of data where we can decompose it into a lower format which contains a significative explanative information. This is the case in mathematics with matrix decomposition, and an example of compressing files.\n",
    "\n",
    "Moreover, reducing the data handled in the modeling part could be interesting for daily professional users. Indeed, assuming the model perform well, it could be boring to insert all the data to assess a loan status. And all the data is not always available for daily uses.\n",
    "\n",
    "Here, a usefull process is to reduce the amount of features to the more relevant one by gradient boosting modeling. It is possible to extract the features importances of a tree model, whether it concerns classification or regression, and pursue our modeling work on the most important features. The question of the threshold is critical. Here I deciced to limit it by the selecting those until the cumulative exceed 90% of the total sum.\n",
    "\n",
    "In practice, the use of gradient boosting implementation depends on the data structure or on the underlying computed processes. Even if **[XGBoost](https://xgboost.readthedocs.io/en/stable/)** is widely \n",
    "used and usually represent the best models in Kaggle, others librairies exist. Here, I decided to use the library **[Catboost](https://catboost.ai/)** for it is inner preprocessing of categorical variables, which is not present in **XGBoost**. Since I have not enough time to encode all categorical features by myself, it is highly interesting. Of course, the evaluation of feature importances of this model will depends on the inner encoding process of **Catboost**, which will be certainly not reproduced in the next step. Additionnaly, I decided to not tune its hyperparameters or run it through several iterations considering the time allowed. The training was perform on small sample of the entire data to simplify this step and not demanding to much on my computer. I use the library **[Shap](https://shap.readthedocs.io/en/latest/index.html)** to get the features importances ; shapely additive explanations values are based on cooperative game theory.\n",
    "\n",
    "An interesting cross-validation would be the expertise of professionals who have intuitive knowledge. Obvsiously, the machine learning tools bring probably a better analysis. But, when we hesitate between two features, for example if two feature have a high correlation between them and a similar feature importance. My personal experience with physicians leads me to not exclude those opportunity of cross-validation. \n",
    "\n",
    "### Note\n",
    "\n",
    "If there is a issue when you run the script, even if the seed is set and must reproduce results, here is a script to run before removing unused features.\n",
    "\n",
    "```python\n",
    "selected_features = ['loan_amnt',\n",
    " 'funded_amnt',\n",
    " 'funded_amnt_inv',\n",
    " 'int_rate',\n",
    " 'fico_range_high',\n",
    " 'total_pymnt',\n",
    " 'total_pymnt_inv',\n",
    " 'total_rec_prncp',\n",
    " 'total_rec_int',\n",
    " 'recoveries',\n",
    " 'last_pymnt_amnt',\n",
    " 'last_fico_range_high',\n",
    " 'last_fico_range_low',\n",
    " 'mths_since_rcnt_il',\n",
    " 'mo_sin_old_rev_tl_op',\n",
    " 'last_credit_pull_d', \n",
    " 'last_pymnt_d']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442784f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gb = df_spark.sample(fraction=0.05, seed=1)\n",
    "print(f\"Number of data: {df_gb.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e255b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gb = df_gb.dropna(subset=['loan_status'])\n",
    "print(f\"Number of data: {df_gb.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1680e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_gb = df_gb.select(all_features)\n",
    "features_collected_gb = features_gb.collect()\n",
    "target_gb = df_gb.select('loan_status')\n",
    "target_collected_gb = target_gb.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d0f7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gb = np.array([list(feature) for feature in features_collected_gb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee730a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gb = np.array([feature['loan_status'] for feature in target_collected_gb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2387d119",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48cb9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a945bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [feature for (feature, dtype) in df_gb.dtypes if dtype=='string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6859dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostClassifier\n",
    "\n",
    "pool = Pool(data=X_gb, label=y_gb, feature_names=all_features, cat_features=categorical_features)\n",
    "\n",
    "catboost_model = CatBoostClassifier(iterations=100)\n",
    "catboost_model.fit(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc2cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "explainer = shap.Explainer(catboost_model)\n",
    "shap_values = explainer.shap_values(X_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8587e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_over_feature = np.sum(np.abs(shap_values), axis=0)\n",
    "feature_importance = pd.DataFrame(data=sum_over_feature, index=all_features, columns=['feature_importance'])\n",
    "feature_importance = feature_importance[feature_importance['feature_importance']>0]\n",
    "feature_importance = feature_importance.sort_values(by='feature_importance', ascending=False)\n",
    "feature_importance.shape\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cbd894",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance['cumulative_sum'] = feature_importance['feature_importance'].cumsum()\n",
    "total_sum = feature_importance['feature_importance'].sum()\n",
    "feature_importance['rate'] = feature_importance['cumulative_sum'] / total_sum\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdd2694",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.9\n",
    "selected_features = feature_importance[feature_importance['rate'] < threshold].index\n",
    "selected_features\n",
    "len(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b87fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = [feature for feature in all_features if feature not in selected_features]\n",
    "features_to_drop\n",
    "len(features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6d755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.drop(*features_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5422d2d0",
   "metadata": {},
   "source": [
    "# Data Cleaning - Encoding\n",
    "\n",
    "Now that 17 features are selected, it is time to clean them in a more appropriate way. To analyse the spark dataframe, the code below allows to highlight the cleaning need of each feature. I recommand you to run it before and after the following cleaning scripts if you would like to better understand it.\n",
    "\n",
    "### Imputation\n",
    "\n",
    "In a advanced version, I would impute nan with a more sophisticated process than by the mean of each feature. This could be executed by a modeling imputation with the use of machine learning models. Among many possibilities, it exists k-nearest neighbors imputations or deep learning-based imputations. Nevertheless, we can observe than there is only few data containing nan so it is not so critical for a quick hands-on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaa72bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import isnan\n",
    "# for feature, dtype in df_spark.dtypes:\n",
    "#     print(\"====================================\")\n",
    "#     print(f\"FEATURE: {feature}\")\n",
    "#     if dtype=='string':\n",
    "#         value_counts = df_spark.groupBy(feature).count()\n",
    "#         value_rate = value_counts.count() / df_spark.count()\n",
    "#         print(f\"Number of distincts values: {value_counts.count()} - {value_rate:.2e} %\")\n",
    "#         value_counts.show()\n",
    "#     nan_count = df_spark.filter(df_spark[feature].isNull() | isnan(df_spark[feature])).count()\n",
    "#     nan_rate = nan_count / df_spark.count()\n",
    "#     print(f\"{nan_count} NaN - {nan_rate:.2e} %\")\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e36c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"====================================\")\n",
    "feature = 'last_pymnt_d'\n",
    "print(f\"FEATURE: {feature}\")\n",
    "value_counts = df_spark.groupBy(feature).count()\n",
    "value_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbace96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "to_float_udf = udf(to_float, FloatType())\n",
    "df_spark = df_spark.withColumn(\"last_fico_range_high\", to_float_udf(df_spark[\"last_fico_range_high\"]))\n",
    "df_spark = df_spark.withColumn(\"last_pymnt_amnt\", to_float_udf(df_spark[\"last_pymnt_amnt\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de72519",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.createOrReplaceTempView(\"lending_club_1\")\n",
    "sql_expression = \"\"\"\n",
    "CASE\n",
    "    WHEN last_pymnt_d LIKE 'Jan-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 0/12\n",
    "    WHEN last_pymnt_d LIKE 'Feb-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 1/12\n",
    "    WHEN last_pymnt_d LIKE 'Mar-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 2/12\n",
    "    WHEN last_pymnt_d LIKE 'Apr-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 3/12\n",
    "    WHEN last_pymnt_d LIKE 'May-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 4/12\n",
    "    WHEN last_pymnt_d LIKE 'Jun-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 5/12\n",
    "    WHEN last_pymnt_d LIKE 'Jul-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 6/12\n",
    "    WHEN last_pymnt_d LIKE 'Aug-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 7/12\n",
    "    WHEN last_pymnt_d LIKE 'Sep-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 8/12\n",
    "    WHEN last_pymnt_d LIKE 'Oct-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 9/12\n",
    "    WHEN last_pymnt_d LIKE 'Nov-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 10/12\n",
    "    WHEN last_pymnt_d LIKE 'Dec-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 11/12\n",
    "    ELSE NULL\n",
    "END AS last_pymnt_d_num\n",
    "\"\"\"\n",
    "df_spark = spark.sql(f\"\"\"\n",
    "SELECT *, {sql_expression}\n",
    "FROM lending_club_1\n",
    "\"\"\")\n",
    "\n",
    "df_spark = df_spark.drop(\"last_pymnt_d\")\n",
    "df_spark = df_spark.withColumnRenamed('last_pymnt_d_num', 'last_pymnt_d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a41c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.createOrReplaceTempView(\"lending_club_2\")\n",
    "sql_expression = \"\"\"\n",
    "CASE\n",
    "    WHEN last_credit_pull_d LIKE 'Jan-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 0/12\n",
    "    WHEN last_credit_pull_d LIKE 'Feb-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 1/12\n",
    "    WHEN last_credit_pull_d LIKE 'Mar-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 2/12\n",
    "    WHEN last_credit_pull_d LIKE 'Apr-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 3/12\n",
    "    WHEN last_credit_pull_d LIKE 'May-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 4/12\n",
    "    WHEN last_credit_pull_d LIKE 'Jun-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 5/12\n",
    "    WHEN last_credit_pull_d LIKE 'Jul-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 6/12\n",
    "    WHEN last_credit_pull_d LIKE 'Aug-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 7/12\n",
    "    WHEN last_credit_pull_d LIKE 'Sep-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 8/12\n",
    "    WHEN last_credit_pull_d LIKE 'Oct-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 9/12\n",
    "    WHEN last_credit_pull_d LIKE 'Nov-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 10/12\n",
    "    WHEN last_credit_pull_d LIKE 'Dec-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 11/12\n",
    "    ELSE NULL\n",
    "END AS last_credit_pull_d_num\n",
    "\"\"\"\n",
    "df_spark = spark.sql(f\"\"\"\n",
    "SELECT *, {sql_expression}\n",
    "FROM lending_club_2\n",
    "\"\"\")\n",
    "\n",
    "df_spark = df_spark.drop(\"last_credit_pull_d\")\n",
    "df_spark = df_spark.withColumnRenamed('last_credit_pull_d_num', 'last_credit_pull_d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a3645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_float_rate = udf(lambda x: float(x.replace('%', '')) / 100, FloatType())\n",
    "df_spark = df_spark.withColumn('int_rate', convert_to_float_rate(df_spark['int_rate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab30efbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "numerical_selected_features = [feature for (feature, dtype) in df_spark.dtypes if (dtype!='string' and feature !='loan_status')]\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=numerical_selected_features,\n",
    "    outputCols=numerical_selected_features\n",
    ").setStrategy(\"mean\")\n",
    "\n",
    "df_spark = imputer.fit(df_spark).transform(df_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3efbe0",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "### Correlation\n",
    "\n",
    "It is obvioulsy interesting to analysis correlation, or even partial correlation, between features to identify whether some selected features could be additionnaly removed. Indeed, there could be an issue while modeling colinear features, which could be observed by a brute correlation equals to 1.\n",
    "\n",
    "Moreover, it is interesting to evaluate the partial correlations by removing the covariance on the different features. It allows to extract the direct relationship between each pair of features. A highly partial correlation could be the origin of an unnecessary dimension modeling. Therefore, the data reduction could be also improved here by removing features included in pair of partial correlation with high values, and which have the lowest features importances, i.e. provided here by gradient boosting. The question of the threshold is still critical here, as it was in the data reduction. \n",
    "\n",
    "\n",
    "### Distribution over the *loan_status* target\n",
    "\n",
    "An other aspect of unrelevant features could be the significative difference between the distribution of features among all the modalities of the target *loan_status*. Here I plot the boxplot of each feature whether it concerns the *Charged Off* or *Fully Paid*, and I run a statistical test to evaluate the null hypothesis that there is a significative difference observing the means of the distinct distribtutions. Sometimes, graphical displays do not match with the statistical test.\n",
    "\n",
    "### Note\n",
    "\n",
    "I have honestyly prioritized the modeling section to return a productive work. This section was therefore performed at the end of the hands-on. So the conclusion it provides will not be applied in the rest of this work. Considering the observation of this section, it would be interesting to evaluate the relevance of considering or not each debatable features in the modeling part. \n",
    "\n",
    "But keep in mind this analysis was processed on a small set from the entire data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f79120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_spark.sample(fraction=0.005, seed=1)\n",
    "print(f\"Number of data: {df.count()}\")\n",
    "df_labeled = df.dropna(subset=['loan_status'])\n",
    "print(f\"Number of labeled data: {df_labeled.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ac9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [feature for feature in list(df_spark.columns) if feature != 'loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eb7743",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_collected = df_labeled.select(features).collect()\n",
    "target_collected = df_labeled.select('loan_status').collect()\n",
    "X_labeled = np.array([list(feature) for feature in features_collected], dtype=float)\n",
    "y_labeled = np.array([feature['loan_status'] for feature in target_collected], dtype=float)\n",
    "y_labeled = y_labeled.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d39536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c30c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.DataFrame(X_labeled.astype(float), columns=features)\n",
    "df_combined['loan_status'] = y_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba5d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = 5\n",
    "num_rows = (len(features) + num_cols - 1) // num_cols\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 4))\n",
    "axes = axes.flatten()\n",
    "for i, feature in enumerate(df_combined.columns):\n",
    "    sns.boxplot(df_combined[feature], ax=axes[i])\n",
    "    axes[i].set_title(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ecfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.pairplot(df_combined[features])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c5a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_combined.corr(numeric_only=False).round(2)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0, cbar_kws={'label': 'Partial Correlation'})\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "pcorr_matrix = df_combined.pcorr().round(2)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(pcorr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0, cbar_kws={'label': 'Partial Correlation'})\n",
    "plt.title('Partial Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5322b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = 6\n",
    "num_rows = 6\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 4))\n",
    "axes = axes.flatten()\n",
    "for i, feature in enumerate(features):\n",
    "    df_fully_paid = df_combined[df_combined['loan_status'] == 0.0][feature]\n",
    "    df_charged_off = df_combined[df_combined['loan_status'] == 1.0][feature]\n",
    "    sns.boxplot(df_fully_paid.tolist(), ax=axes[2*i], color='C1')\n",
    "    axes[2*i].set_title(f\"Fully Paid\\n{feature}\")\n",
    "    sns.boxplot(df_charged_off.tolist(), ax=axes[2*i+1], color='C2')\n",
    "    axes[2*i+1].set_title(f\"Charged Off\\n{feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9137add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, f_oneway\n",
    "\n",
    "fully_paid = df_combined[df_combined['loan_status'] == 0][features]\n",
    "charged_off = df_combined[df_combined['loan_status'] == 1][features]\n",
    "t_stat, p_val = ttest_ind(fully_paid, charged_off)\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    print(f\"Feature: {feature}\")\n",
    "    print(f\"\\tT-statistic: {t_stat[i]}\")\n",
    "    print(f\"\\tP-value: {p_val[i]}\")\n",
    "    if p_val[i] < 0.05:\n",
    "        print(\"\\tReject the null hyptothesis ~ Significant difference between the means of binary 'loan_status'\")\n",
    "    else:\n",
    "        print(\"\\tFail to reject the null hyptothesis ~ No significant difference between the means of binary 'loan_status'\")\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b745349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan\n",
    "df_nan = df.filter(isnan(df['loan_status']))\n",
    "print(f\"Number of NaN data: {df_nan.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_collected = df_nan.select(features).collect()\n",
    "X_unlabeled = np.array([list(feature) for feature in features_collected], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b383f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_ = np.unique(y_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68caadf",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "When it comes to choose a machine learnning classifier to predict *loan_status*, two main classes are well known to perform good evaluation: deep neural networks and gradient boosting. *XGBoost* is one of the best model in common Kaggle competition. Several articles compare their performances:\n",
    "\n",
    "**Article**: [Why do tree-based models still outperform deep learning on typical tabular data?](https://proceedings.neurips.cc/paper_files/paper/2022/hash/0378c7692da36807bdec87ab043cdadc-Abstract-Datasets_and_Benchmarks.html)\n",
    "\n",
    "**Article**: [Credit Risk Assessment based on Gradient Boosting Decision Tree](https://www.sciencedirect.com/science/article/pii/S1877050920315842)\n",
    "\n",
    "**Article**: [Deep Learning vs. Gradient Boosting: Benchmarking state-of-the-art machine learning algorithms for credit scoring](https://arxiv.org/abs/2205.10535)\n",
    "\n",
    "However, to illustrate my abilities in this hands-on, I choose to apply a deep multilayer perceptron network to classify loans since I already use a gradient boosting model for data reduction. Initially, since the goal of this work is to target only two *loan_status* among a dozen, I compare it to spam filtered in Google. Even if it has evolved, I keep in mind that the underlying process is related to semi-supervised classification. This involves unlabeled and labeled data. \n",
    "\n",
    "Additionnaly, even if I have already work on TensorFlow and the high level Keras framework, I choose to apply Pytorch to gain some times since I am habit to work on it in my current schedule. I have added several elements to its model to bring a little more complexity.\n",
    "\n",
    "### Scalability\n",
    "\n",
    "The rate of the train set could be filled when initializing the model. Nevertheless, I recommand to use between 30% and 15% of the entire data to keep a reliable test set to evaluate properly the model. Here, the amount of data is too large for a quick hands-on. You can see that I have also test my model on larger sample of the spark dataframe than the training sample, and the evaluation still pretty nice. If I have more time, I would probably try to take a very larger size of data. \n",
    "\n",
    "### Conformal predicted intervals\n",
    "\n",
    "**[Mapie](https://mapie.readthedocs.io/en/latest/)** library provides conformal predicted intervals, whether it concerns regression or classification. However, it could be only applied on **Scikit-learn** library format ; explaining why I must to implement a intermediate class named **MLPEstimatorSklearn**. Unfortunaly, I still have a issue while I deliver this hands-on: while *fordward_proba* effectively provides a float, the inner process of **MapieClassifier** probably translate it into a boolean since float values could be extremely near from both 0 and 1. And I suppose their is a process that translate 0 and 1 to boolean, like in C language. Something like this, I did not deepen this issue.\n",
    "\n",
    "**Article**: [Conformal Prediction Sets for Ordinal Classification](https://proceedings.neurips.cc/paper_files/paper/2023/hash/029f699912bf3db747fe110948cc6169-Abstract-Conference.html)\n",
    "\n",
    "**Article**: [Conjugate Conformal Prediction for Online Binary Classification](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=77fbaec8faa58fc547de29082538b16c328545df)\n",
    "\n",
    "### Hyperparameters tuning\n",
    "\n",
    "Inner hyperparameters tuning whether it is by calling randomized or bayes optimization. I fill a range for each hyperparameter which could be debatable. I have set string in hidden_layer_size hyperparameter which represent the hidden architecture of the MLP classifier since bayes search could not accept a tuple of tuple for its range, even with extra functions from **Skopt** such as *Optional*. This is why I handle it through tuple in the low-level implementation of **MLP**. Unfornutalely, I also have an issue with bayes search. While randomized search work, bayesian tuning seems to be not up to date to **Numpy** library. And I was not able to find a quick solution within the time allowed. If you want to optimize hyperparamaters of the model, please run the method *randomized_search* before fitting the model. Note also that it could not explore the initial hyperparameters settings, which is already highly interesting. Nevertheless, it is also highly possible to find a better architecture, for example, with enough iterations of search. Even if the exploration is limited here, as a reminder, there is not a suitable architecture for any problems since there is not prior strategy to find the optimal architecture.\n",
    "\n",
    "### Interprating method through integrated gradients\n",
    "\n",
    "Deep neural networks are advanced black-box processes which are difficult to interpret. However, it exists some methods to bring interpration on their well performance. One of the most interesting one is integrated gradients since it checks many axioms. By interpolating gradients, it is possible to identify which features is the most important for the attribution of a deep neural netwoks, whether it concerns classification or regression for example, even in NLP, Computer Vision fields. Here I use **[Captum](https://captum.ai/)** libraby which is linked to **Pytorch** to accelerate its computing. It involves NVIDIA libraries to parallel computing, explaining the quite large size of the docker image of my trained MLP binary classifier. I do not recommend to run *compute_integreated_gradients*  on a large amount to not freeze your computer, which was my case. Nevertheless, it allows to provides the importances of each feature toward the attribution of the *loan_status* attribution. I decided to normalized it in min max standardization to better catch the importances, while the original value is extremely small.\n",
    "\n",
    "**Article** : [Axiomatic Attribution for Deep Networks](http://proceedings.mlr.press/v70/sundararajan17a.html)\n",
    "\n",
    "\n",
    "However, Kolmogorov Arnold networks are a well alternative to black-box neural networks. Without going deepen to desrible the KAN models since it is not the subject of its hands-on, it computes activation functions by the use of splines, instead of computing weights and biaises. It could be very interesting if you would like to get a \"generic\" function, linking intputs to outputs. It generally needs a smaller architecture to provides same evaluation than a black-box neural networks. A main interest coulb be explain when physician would like to identify a generic function of a unknown process, for example.\n",
    "\n",
    "\n",
    "### Note\n",
    "\n",
    "I have tried different strategies to check the performance of the model. Even if I know it is better to target a single neuron in the output layer in case of binary classifier, I also tried to used two neurons. Why I did this? Because I hesitated on the well approach to return probabilities of the two class. I know that softmax activation function is the activation function to apply to return probabilities for each neuron in a multi-class classifier. While sigmoid function also returns value between 0 and 1, I honestly hesitated whether or not is it appropriate to return the probability of the class 1, i.e. *Charged Off* here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b5568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from captum.attr import IntegratedGradients\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from mapie.classification import MapieClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from skopt import BayesSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layer_sizes, activation_name, p_dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        if isinstance(self.hidden_layer_sizes, str):\n",
    "            self.hidden_layer_sizes = eval(self.hidden_layer_sizes)\n",
    "        self.activation_name = activation_name\n",
    "        self.p_dropout = p_dropout\n",
    "        if activation_name == \"Relu\":\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation_name == \"Sigmoid\":\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation_name == \"Softmax\":\n",
    "            self.activation = nn.Softmax(dim=1)\n",
    "        elif activation_name == \"Tanh\":\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation_name == \"Leaky_relu\":\n",
    "            self.activation = nn.LeakyReLU()\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported activation: {self.activation_name}')\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(self.input_size, self.hidden_layer_sizes[0]))\n",
    "        layers.append(self.activation)\n",
    "        for i in range(len(self.hidden_layer_sizes) - 1):\n",
    "            layers.append(nn.Linear(self.hidden_layer_sizes[i], self.hidden_layer_sizes[i + 1]))\n",
    "            layers.append(self.activation)\n",
    "            layers.append(nn.Dropout(p=self.p_dropout))\n",
    "        layers.append(nn.Linear(self.hidden_layer_sizes[-1], self.output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward_proba(self, X):\n",
    "        output = self.model(X)\n",
    "        output = self.sigmoid(output)\n",
    "        return output.float()\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        output = self.model(X)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class MLPEstimatorSklearn():\n",
    "    def __init__(self, **params):\n",
    "        self.input_size = params.get(\"input_size\")\n",
    "        self.output_size = params.get(\"output_size\")\n",
    "        self.hidden_layer_sizes = params.get(\"hidden_layer_sizes\", (60, 60))\n",
    "        self.activation_name = params.get(\"activation_name\", \"Relu\")\n",
    "        self.loss = params.get(\"loss\", \"binary_cross_entropy\")\n",
    "        self.optimizer_name = params.get(\"optimizer_name\", \"Adam\")\n",
    "        self.learning_rate = params.get(\"learning_rate\", 1e-3)\n",
    "        self.batch_size = params.get(\"batch_size\", 50)\n",
    "        self.weight_decay = params.get(\"weight_decay\", 0)\n",
    "        self.p_dropout = params.get(\"p_dropout\", 0.2)\n",
    "        self.early_stopping = params.get(\"early_stopping\", True)\n",
    "        self.epochs = params.get(\"epochs\", 200)\n",
    "        self.patience = params.get(\"patience\", 10)\n",
    "        self.verbose = params.get(\"verbose\", True)\n",
    "        self.classes_ = classes_\n",
    "\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = MLP(self.input_size, self.output_size, self.hidden_layer_sizes, self.activation_name, self.p_dropout).to(self.device)\n",
    "\n",
    "        if self.loss == \"binary_cross_entropy\":\n",
    "            self.criterion = nn.BCELoss()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss: {self.loss}\")\n",
    "\n",
    "        if self.optimizer_name == \"SGD\":\n",
    "            self.optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate, momentum=0.9, weight_decay=self.weight_decay)\n",
    "        elif self.optimizer_name == \"Adam\":\n",
    "            self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer: {self.optimizer}\")\n",
    "            \n",
    "    def next_batch(self, inputs, targets, batchSize):\n",
    "        inputs_tensor = torch.from_numpy(inputs).float()\n",
    "        targets_tensor = torch.from_numpy(targets).float().unsqueeze(1)\n",
    "        for i in range(0, inputs_tensor.shape[0], batchSize):\n",
    "            yield (inputs_tensor[i:i + batchSize], targets_tensor[i:i + batchSize])\n",
    "\n",
    "    def augment_data(self, X_unlabeled, noise_level=0.1):\n",
    "        noise = noise_level * torch.randn_like(X_unlabeled)\n",
    "        return X_unlabeled + noise\n",
    "            \n",
    "    def fit(self, X, y, X_unlabeled=None):\n",
    "        self.classes_ = np.unique(y)\n",
    "        running_losses_1 = list()\n",
    "        if self.early_stopping:\n",
    "            best_loss = float('inf')\n",
    "            count = 0\n",
    "        if self.verbose:\n",
    "            epoch_iterator_1 = tqdm(range(self.epochs), desc=\"Supervised training ; epochs\", unit=\"epoch\")\n",
    "        else:\n",
    "            epoch_iterator_1 = range(self.epochs)\n",
    "        for epoch in epoch_iterator_1:\n",
    "            samples = 0\n",
    "            train_loss = 0.0\n",
    "            self.model.train(True)\n",
    "            for i, (batchX, batchY) in enumerate(self.next_batch(X, y, self.batch_size)):\n",
    "                batchX = batchX.to(self.device)\n",
    "                batchY = batchY.to(self.device)\n",
    "                batchY.requires_grad = True\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(batchX)\n",
    "                loss = self.criterion(outputs, batchY)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                samples += batchY.size(0)\n",
    "            running_loss = train_loss / samples\n",
    "            running_losses_1.append(running_loss)\n",
    "            if self.verbose:\n",
    "                epoch_iterator_1.set_postfix(train_loss=running_loss)\n",
    "            if self.early_stopping:\n",
    "                if running_loss < best_loss:\n",
    "                    best_loss = running_loss\n",
    "                    count = 0\n",
    "                else:\n",
    "                    count += 1\n",
    "                if count >= self.patience:\n",
    "                    break\n",
    "        if self.verbose:\n",
    "            epoch_iterator_1.close()\n",
    "        self.running_losses_1 = [loss for loss in running_losses_1 if loss <= best_loss]\n",
    "        if X_unlabeled is not None:\n",
    "            best_loss = float('inf')\n",
    "            running_losses_2 = list()\n",
    "            X_unlabeled = torch.from_numpy(X_unlabeled).float()\n",
    "            augmented_X_unlabeled = self.augment_data(X_unlabeled)\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                pseudo_labels = self.model(X_unlabeled)\n",
    "                pseudo_labels = (pseudo_labels > 0.5).float().squeeze()\n",
    "            \n",
    "            X_combined = torch.cat((torch.from_numpy(X).float(), augmented_X_unlabeled.cpu()), 0).to(self.device)\n",
    "            y_combined = torch.cat((torch.from_numpy(y).float().squeeze(), pseudo_labels), 0).to(self.device)\n",
    "\n",
    "            if self.verbose:\n",
    "                epoch_iterator_2 = tqdm(range(self.epochs), desc=\"Semi-supervised training ; epochs\", unit=\"epoch\")\n",
    "            else:\n",
    "                epoch_iterator_2 = range(self.epochs)\n",
    "            for epoch in epoch_iterator_2:\n",
    "                samples = 0\n",
    "                train_loss = 0.0\n",
    "                self.model.train(True)\n",
    "                dataset = TensorDataset(X_combined, y_combined)\n",
    "                for i, (batchX, batchY) in enumerate(self.next_batch(X, y, self.batch_size)):\n",
    "                    batchX = batchX.to(self.device)\n",
    "                    batchY = batchY.to(self.device)\n",
    "                    batchY.requires_grad = True\n",
    "                    self.optimizer.zero_grad()\n",
    "                    outputs = self.model(batchX)\n",
    "                    loss = self.criterion(outputs, batchY)\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    train_loss += loss.item()\n",
    "                    samples += batchY.size(0)\n",
    "                running_loss = train_loss / samples\n",
    "                running_losses_2.append(running_loss)\n",
    "                if self.verbose:\n",
    "                    epoch_iterator_2.set_postfix(train_loss=running_loss)\n",
    "                if self.early_stopping:\n",
    "                    if running_loss < best_loss:\n",
    "                        best_loss = running_loss\n",
    "                        count = 0\n",
    "                    else:\n",
    "                        count += 1\n",
    "                    if count >= self.patience:\n",
    "                        break\n",
    "            if self.verbose:\n",
    "                epoch_iterator_2.close()\n",
    "            self.running_losses_2 = [loss for loss in running_losses_2 if loss <= best_loss]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        X = torch.from_numpy(X).float().to(self.device)\n",
    "        y_pred = self.model.forward(X)\n",
    "        if self.device == \"cpu\":\n",
    "            y_pred = y_pred.cpu().detach().numpy()\n",
    "        else:\n",
    "            y_pred = y_pred.detach().numpy()\n",
    "        return (y_pred > 0.5).astype(int)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        self.model.eval()\n",
    "        X = torch.from_numpy(X).float().to(self.device)\n",
    "        y_proba = self.model.forward_proba(X)\n",
    "        if self.device == \"cpu\":\n",
    "            y_proba = y_proba.cpu().detach().numpy().astype(float)\n",
    "        else:\n",
    "            y_proba = y_proba.detach().numpy().astype(float)\n",
    "        y_proba = y_proba.squeeze().astype(np.float32)\n",
    "        return np.column_stack((1 - y_proba, y_proba))\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        params = {\n",
    "            \"input_size\": self.input_size,\n",
    "            \"output_size\": self.output_size,\n",
    "            \"hidden_layer_sizes\": self.hidden_layer_sizes,\n",
    "            \"activation_name\": self.activation_name,\n",
    "            \"loss\": self.loss,\n",
    "            \"optimizer_name\": self.optimizer_name,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"weight_decay\": self.weight_decay,\n",
    "            \"p_dropout\": self.p_dropout,\n",
    "            \"early_stopping\": self.early_stopping,\n",
    "            \"epochs\": self.epochs,\n",
    "            \"patience\": self.patience,\n",
    "            \"verbose\": self.verbose\n",
    "        }\n",
    "        return params\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            if hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "    def get_mlp(self):\n",
    "        return self.model\n",
    "    \n",
    "class MLPBinaryClassifier():\n",
    "    def __init__(self, X, y, split_test, X_unlabeled=None, **params):\n",
    "        self.model = MLPEstimatorSklearn(**params)\n",
    "        self.X = X\n",
    "        self.X_unlabeled = X_unlabeled\n",
    "        self.y = y\n",
    "        \n",
    "        self.y = MLPBinaryClassifier.float_to_class(self.y).ravel()\n",
    "        \n",
    "        self.split_test = split_test\n",
    "        self.split_data()\n",
    "        \n",
    "        self.standardize(self.X_train_cal)\n",
    "        self.X_train_standard = self.standardize_X(self.X_train)\n",
    "        self.X_cal_standard = self.standardize_X(self.X_cal)\n",
    "        if isinstance(self.X_unlabeled, np.ndarray):\n",
    "            self.X_unlabeled_standard = self.standardize_X(self.X_unlabeled)\n",
    "        else :\n",
    "            self.X_unlabeled_standard = None\n",
    "        self.y_train_standard = self.y_train\n",
    "        self.y_cal_standard = self.y_cal.reshape(-1,1)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def float_to_class(y):\n",
    "        threshold = 0.5\n",
    "        return (y >= threshold).astype(int)\n",
    "    \n",
    "    def split_data(self):\n",
    "        self.X_train_cal, self.X_test, self.y_train_cal, self.y_test = train_test_split(\n",
    "            self.X, self.y, test_size=self.split_test, shuffle=True, random_state=1, stratify=self.y)\n",
    "        self.X_train, self.X_cal, self.y_train, self.y_cal = train_test_split(\n",
    "            self.X_train_cal, self.y_train_cal, test_size=0.25, shuffle=True, random_state=1, stratify=self.y_train_cal)\n",
    "\n",
    "    def standardize(self, X):\n",
    "        self.scaler_X_train = StandardScaler()\n",
    "        self.scaler_X_train.fit(X)         \n",
    "\n",
    "\n",
    "    def standardize_X(self, X):\n",
    "        X_new = self.scaler_X_train.transform(X)\n",
    "        return X_new\n",
    "    \n",
    "\n",
    "\n",
    "    def bayes_search(self, param_bayes, n_iter, n_points=1, cv=5, scoring='accuracy',\n",
    "                 verbose=3, n_jobs=1) :\n",
    "        cv = StratifiedKFold(n_splits=cv, shuffle=True, random_state=1)\n",
    "        bayes_search = BayesSearchCV(self.model, param_bayes, n_iter=n_iter,\n",
    "                                     n_points=n_points, cv=cv, scoring=scoring,\n",
    "                                     verbose=verbose, return_train_score=True,\n",
    "                                     n_jobs=n_jobs, random_state=1)\n",
    "        bayes_search.fit(self.X_train_standard, self.y_train_standard)\n",
    "        results_df = pd.DataFrame(bayes_search.cv_results_)\n",
    "        self.model = bayes_search.best_estimator_\n",
    "        print(f'Best hyperparameters bayes search : {bayes_search.best_params_}')\n",
    "        return results_df\n",
    "\n",
    "    def randomized_search(self, param_randomized, n_iter, cv=5, scoring='accuracy',\n",
    "                      verbose=3, n_jobs=1) :\n",
    "        cv = StratifiedKFold(n_splits=cv, shuffle=True, random_state=1)\n",
    "        randomized_search = RandomizedSearchCV(self.model, param_randomized,\n",
    "                                               n_iter=n_iter, cv=cv, scoring=scoring,\n",
    "                                               verbose=verbose, return_train_score=True,\n",
    "                                               n_jobs=n_jobs, random_state=1)\n",
    "        randomized_search.fit(self.X_train_standard, self.y_train_standard)\n",
    "        results_df = pd.DataFrame(randomized_search.cv_results_)\n",
    "        self.model = randomized_search.best_estimator_\n",
    "        print(f'Best hyperparameters randomized search : {randomized_search.best_params_}')\n",
    "        return results_df\n",
    "\n",
    "    def fit(self, method=\"lac\"):\n",
    "        self.model.fit(self.X_train_standard, self.y_train_standard, self.X_unlabeled_standard)\n",
    "        self.model_mapie = MapieClassifier(estimator=self.model, cv=\"prefit\", method=method)\n",
    "        self.model_mapie.fit(self.X_cal_standard, self.y_cal_standard)\n",
    "\n",
    "    def predict(self, X, alpha=0.05):\n",
    "        X_standard = self.standardize_X(X)\n",
    "        y_pred, y_ps = self.model_mapie.predict(X_standard, alpha=alpha)\n",
    "        return y_pred, y_ps\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_metrics(metric, y_true, y_pred):\n",
    "        y_pred = MLPBinaryClassifier.float_to_class(y_pred)\n",
    "        accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "        precision = metrics.precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        recall = metrics.recall_score(y_true, y_pred, average='weighted')\n",
    "        f1 = metrics.f1_score(y_true, y_pred, average='weighted')\n",
    "        metrics_dict = {\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1': f1,\n",
    "        }\n",
    "        if metric != 'all':\n",
    "            metrics_dict = {metric: metrics_dict[metric]}\n",
    "        return metrics_dict\n",
    "\n",
    "\n",
    "    def model_performance(self, metric='all'):\n",
    "        y_pred_train, _ = self.predict(self.X_train)\n",
    "        scores_train = MLPBinaryClassifier.compute_metrics(metric, self.y_train, y_pred_train)\n",
    "        y_pred_test, _ = self.predict(self.X_test)\n",
    "        scores_test = MLPBinaryClassifier.compute_metrics(metric, self.y_test, y_pred_test)\n",
    "        data = {}\n",
    "        for key, value in scores_train.items():\n",
    "            data['Train Set - '+key] = [value]\n",
    "        for key, value in scores_test.items():\n",
    "            data['Test Set - '+key] = [value]\n",
    "        df_scores = pd.DataFrame(data=data).T\n",
    "        df_scores.columns = ['Scores']\n",
    "        return df_scores\n",
    "\n",
    "    def model_performance_test(self, X_test, y_test, metric='all'):\n",
    "        y_pred_test, _ = self.predict(X_test)\n",
    "        scores_test = MLPBinaryClassifier.compute_metrics(metric, y_test, y_pred_test)\n",
    "        data = {}\n",
    "        for key, value in scores_test.items():\n",
    "            data['Test Set - '+key] = [value]\n",
    "        df_scores = pd.DataFrame(data=data).T\n",
    "        df_scores.columns = ['Scores']\n",
    "        return df_scores\n",
    "\n",
    "    def receiver_operating_characteristics(self):\n",
    "        y_pred_test, _ = self.predict(self.X_test)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(self.y_test, y_pred_test)\n",
    "        plt.plot(fpr, tpr)\n",
    "        plt.title(\"Receiver Operating Characteristics\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.show()\n",
    "\n",
    "    def compute_integrated_gradients(self, X, baseline=None, steps=50):\n",
    "        def preprocess_input(X):\n",
    "            return torch.tensor(X, dtype=torch.float32)\n",
    "        input_tensor = preprocess_input(X)\n",
    "        if baseline is None:\n",
    "            baseline = torch.zeros_like(input_tensor)\n",
    "        integrated_gradients = IntegratedGradients(self.model.get_mlp())\n",
    "        attributions = integrated_gradients.attribute(input_tensor, baseline, target=0, n_steps=steps)\n",
    "        attributions_df = pd.DataFrame(attributions.cpu().detach().numpy(), columns=features)\n",
    "        avg_attributions = attributions_df.mean(axis=0)\n",
    "        avg_abs_attributions = avg_attributions.abs()\n",
    "        def custom_minmax_scaler(data, feature_range=(0, 100)):\n",
    "            min_val = np.min(data)\n",
    "            max_val = np.max(data)\n",
    "            if max_val - min_val == 0:\n",
    "                return np.zeros_like(data) if feature_range[0] == 0 else np.full_like(data, feature_range[0])\n",
    "            scale = (feature_range[1] - feature_range[0]) / (max_val - min_val)\n",
    "            min_range = feature_range[0]\n",
    "            scaled_data = scale * (data - min_val) + min_range\n",
    "            return scaled_data\n",
    "        normalized_data = custom_minmax_scaler(avg_abs_attributions.values.reshape(-1, 1)).astype(float)\n",
    "        np.set_printoptions(suppress=True, precision=2)\n",
    "        normalized_attributions = pd.DataFrame(normalized_data, columns=['attribution'], index=features)\n",
    "        sorted_attributions = normalized_attributions.sort_values(by=\"attribution\", ascending=False)\n",
    "        return sorted_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab236126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import loguniform, uniform\n",
    "from skopt.space import Real\n",
    "\n",
    "params = {    \n",
    "    \"init\" : {\n",
    "        \"input_size\" : len(features),\n",
    "        \"output_size\" : 1,\n",
    "        \"hidden_layer_sizes\" : (60,60),\n",
    "        \"activation_name\" : \"Relu\",\n",
    "        \"optimizer_name\" : \"Adam\",\n",
    "        \"learning_rate\" : 1e-3,\n",
    "        \"batch_size\" : 50,\n",
    "        \"weight_decay\" : 0,\n",
    "        \"p_dropout\" : 0.3,\n",
    "        \"loss\" : \"binary_cross_entropy\",\n",
    "        \"early_stopping\" : True,\n",
    "        \"epochs\" : 200,\n",
    "        \"patience\" : 10,\n",
    "        \"verbose\" : True\n",
    "    },\n",
    "    \"randomized\": {\n",
    "        \"hidden_layer_sizes\" : [(10,),(50,),(100,),(10,10),(50,50),(60,60),(100,50),(100,100),(100,50,25)],\n",
    "        \"activation_name\" :  [\"Relu\", \"Sigmoid\", \"Tanh\", \"Leaky_relu\", \"Softmax\"],\n",
    "        \"learning_rate\" : loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\" : list(np.arange(10,500, 10)),\n",
    "        \"optimizer_name\" : [\"Adam\", \"SGD\"],\n",
    "        \"alpha\" : np.logspace(-3,0,19),\n",
    "        \"weight_decay\" : loguniform(1e-5, 1),\n",
    "        \"p_dropout\" : uniform(0, 0.4)   \n",
    "    },\n",
    "    \"bayes\": {\n",
    "        \"hidden_layer_sizes\" : [\"(10,)\",\"(50,)\",\"(100,)\",\"(10,10)\",\"(50,50)\",\"(60,60)\",\"(100,50)\",\"(100,100)\",\"(100,50,25)\"],\n",
    "        \"activation_name\" :  [\"Relu\", \"Sigmoid\", \"Tanh\", \"Leaky_relu\", \"Softmax\"],\n",
    "        \"learning_rate\" : Real(1e-4, 1e-1, prior='log-uniform'),\n",
    "        \"batch_size\" : list(np.arange(10,500, 10)),\n",
    "        \"optimizer_name\" : [\"Adam\", \"SGD\"],\n",
    "        \"alpha\" : np.logspace(-3,0,19),\n",
    "        \"weight_decay\" : Real(1e-5, 1, prior='log-uniform'),\n",
    "        \"p_dropout\" : Real(0, 0.4, prior='uniform')\n",
    "    }\n",
    "}\n",
    "\n",
    "model_mlp = MLPBinaryClassifier(X=X_labeled, y=y_labeled, X_unlabeled=X_unlabeled, split_test=0.2, **params[\"init\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02818466",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 5\n",
    "n_points=1\n",
    "cv=5\n",
    "scoring='accuracy'\n",
    "verbose=3\n",
    "n_jobs=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5845bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_mlp.bayes_search(\n",
    "#     param_bayes=params['bayes'],\n",
    "#     n_iter=n_iter,\n",
    "#     n_points=n_points,\n",
    "#     cv=cv,\n",
    "#     scoring=scoring,\n",
    "#     n_jobs=n_jobs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63527ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_mlp.randomized_search(\n",
    "#     param_randomized=params['randomized'],\n",
    "#     n_iter=n_iter,\n",
    "#     cv=cv,\n",
    "#     scoring=scoring,\n",
    "#     n_jobs=n_jobs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407024f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp.model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6b9200",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521018d1",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "\n",
    "\n",
    "For a classification problem, few elements must be identified:\n",
    "\n",
    "- **TP** (True Positives) is the number of correctly classified *Charged Off* loans.\n",
    "- **TN** (True Negatives) is the number of correctly classified *Fully Paid* loans.\n",
    "- **FP** (False Positives) is the number of *Fully Paid* loans incorrectly classified as 'Charged Off'.\n",
    "- **FN** (False Negatives) is the number of *Charged Off* loans incorrectly classified as 'Fully Paid'.\n",
    "\n",
    "Here a metrics to evaluate the performance of the model:\n",
    "\n",
    "*Precision, Recall and F1Score provide a better understanding of how well the model is performing in identifying the *Charge Off* loans, the minority target of the dataset.*\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "This metrics is the main metric to evaluate the model through the hyperparameters search.\n",
    "\n",
    "$$ Accuracy = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    "\n",
    "### Precision\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "### Recall - Sensitivity\n",
    "\n",
    "$$ Accuracy = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "### F1Score\n",
    "\n",
    "$$ F1Score = 2 * \\frac{Precision * Recall}{Precision + Recall} $$\n",
    "\n",
    "### ROC-AUC\n",
    "\n",
    "The area under the receiver operation characteristic curve is a single value summarizing the overall ability of the model.\n",
    "\n",
    "### Observations\n",
    "\n",
    "The trained model well performed a binary classification on the dataset since metrics are near from 1, i.e. higher than 0,99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedc829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp.model_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4212cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp.receiver_operating_characteristics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac53482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_spark.sample(fraction=0.05, seed=30)\n",
    "df_test = df_test.dropna(subset=['loan_status'])\n",
    "features_collected = df_test.select(features).collect()\n",
    "X_test = np.array([list(feature) for feature in features_collected])\n",
    "target_collected = df_test.select('loan_status').collect()\n",
    "y_test = np.array([feature['loan_status'] for feature in target_collected])\n",
    "y_test = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09624bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp.model_performance_test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a3b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_value_collected = df_spark.sample(withReplacement=False, fraction=0.0001, seed=1).limit(1).collect()[0]\n",
    "single_value = np.array([value for key, value in single_value_collected.asDict().items() if key != 'loan_status']).reshape(1,-1).astype(float)\n",
    "single_value_target = np.array([value for key, value in single_value_collected.asDict().items() if key == 'loan_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02be47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp.compute_integrated_gradients(single_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea02de2d",
   "metadata": {},
   "source": [
    "# Data Visualization\n",
    "\n",
    "If I have more time, I would like to display the prediction areas of the classifier in multiple 2D or 3D graphics, taking different features in axes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198bfd39",
   "metadata": {},
   "source": [
    "# Saving \n",
    "\n",
    "I saved a trained MLP classifier model with initial configuration for my docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0412b1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('app/lending_club_mlp_binary_classifier.pkl', 'wb') as file:\n",
    "    pickle.dump(model_mlp, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f6e7c",
   "metadata": {},
   "source": [
    "# App\n",
    "\n",
    "The docker image of the app is located [here](https://hub.docker.com/repository/docker/yanncauchepin/lendingclub/general).\n",
    "\n",
    "I realized that I have not handle the case where one of the feature filled to describe the data is NaN. I would probably code a inner function of the model to process an imputation with a more advanced method than just setting the mean value of the feature.\n",
    "\n",
    "Due to an additionnal line that I forgot to removed in the past, the application and its model do not handle the feature *last_pymnt_d* while it is well selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f616892",
   "metadata": {},
   "source": [
    "# Thank you\n",
    "\n",
    "I would like to thank you for taking the time to read this hands-on! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
