{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a499894c",
   "metadata": {},
   "source": [
    "# Yann Cauchepin\n",
    "\n",
    "Hi, here is my documented jupyter notebook which responds to the hands-on test. I tried to make it fast to compute to save your time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae1b09e",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "Before even running the following script, please follow the first steps:\n",
    "\n",
    "- [ ] Installing the necessary librairies. You can comment the next script to avoid biding your time.\n",
    "\n",
    "- [ ] Replacing dataset path toward your correct local repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a923e49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scipy in /home/yanncauchepin/.local/lib/python3.10/site-packages (1.11.1)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from scipy) (1.24.3)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /home/yanncauchepin/.local/lib/python3.10/site-packages (4.64.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/yanncauchepin/.local/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/yanncauchepin/.local/lib/python3.10/site-packages (1.24.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /home/yanncauchepin/.local/lib/python3.10/site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: catboost in /home/yanncauchepin/.local/lib/python3.10/site-packages (1.1.1)\n",
      "Requirement already satisfied: matplotlib in /usr/lib/python3/dist-packages (from catboost) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from catboost) (1.24.3)\n",
      "Requirement already satisfied: graphviz in /home/yanncauchepin/.local/lib/python3.10/site-packages (from catboost) (0.20.1)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from catboost) (1.5.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: scipy in /home/yanncauchepin/.local/lib/python3.10/site-packages (from catboost) (1.11.1)\n",
      "Requirement already satisfied: plotly in /home/yanncauchepin/.local/lib/python3.10/site-packages (from catboost) (5.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from pandas>=0.24.0->catboost) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from plotly->catboost) (8.1.0)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: shap in /home/yanncauchepin/.local/lib/python3.10/site-packages (0.45.1)\n",
      "Requirement already satisfied: scipy in /home/yanncauchepin/.local/lib/python3.10/site-packages (from shap) (1.11.1)\n",
      "Requirement already satisfied: packaging>20.9 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from shap) (23.2)\n",
      "Requirement already satisfied: numpy in /home/yanncauchepin/.local/lib/python3.10/site-packages (from shap) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in /home/yanncauchepin/.local/lib/python3.10/site-packages (from shap) (1.2.0)\n",
      "Requirement already satisfied: numba in /home/yanncauchepin/.local/lib/python3.10/site-packages (from shap) (0.60.0)\n",
      "Requirement already satisfied: pandas in /home/yanncauchepin/.local/lib/python3.10/site-packages (from shap) (1.5.2)\n",
      "Requirement already satisfied: cloudpickle in /home/yanncauchepin/.local/lib/python3.10/site-packages (from shap) (2.2.0)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from shap) (4.64.1)\n",
      "Requirement already satisfied: slicer==0.0.8 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from numba->shap) (0.43.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from pandas->shap) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from scikit-learn->shap) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from scikit-learn->shap) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pingouin in /home/yanncauchepin/.local/lib/python3.10/site-packages (0.5.4)\n",
      "Requirement already satisfied: seaborn in /home/yanncauchepin/.local/lib/python3.10/site-packages (from pingouin) (0.12.1)\n",
      "Requirement already satisfied: pandas>=1.5 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from pingouin) (1.5.2)\n",
      "Requirement already satisfied: statsmodels in /home/yanncauchepin/.local/lib/python3.10/site-packages (from pingouin) (0.13.5)\n",
      "Requirement already satisfied: tabulate in /home/yanncauchepin/.local/lib/python3.10/site-packages (from pingouin) (0.9.0)\n",
      "Requirement already satisfied: scipy in /home/yanncauchepin/.local/lib/python3.10/site-packages (from pingouin) (1.11.1)\n",
      "Requirement already satisfied: scikit-learn in /home/yanncauchepin/.local/lib/python3.10/site-packages (from pingouin) (1.2.0)\n",
      "Requirement already satisfied: matplotlib in /usr/lib/python3/dist-packages (from pingouin) (3.5.1)\n",
      "Requirement already satisfied: numpy in /home/yanncauchepin/.local/lib/python3.10/site-packages (from pingouin) (1.24.3)\n",
      "Requirement already satisfied: pandas-flavor in /home/yanncauchepin/.local/lib/python3.10/site-packages (from pingouin) (0.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from pandas>=1.5->pingouin) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from pandas>=1.5->pingouin) (2022.7.1)\n",
      "Requirement already satisfied: xarray in /home/yanncauchepin/.local/lib/python3.10/site-packages (from pandas-flavor->pingouin) (2022.12.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from scikit-learn->pingouin) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from scikit-learn->pingouin) (3.1.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from statsmodels->pingouin) (0.5.3)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from statsmodels->pingouin) (23.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from patsy>=0.5.2->statsmodels->pingouin) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in /home/yanncauchepin/.local/lib/python3.10/site-packages (0.12.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from seaborn) (1.24.3)\n",
      "Requirement already satisfied: pandas>=0.25 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from seaborn) (1.5.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/lib/python3/dist-packages (from seaborn) (3.5.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas>=0.25->seaborn) (1.16.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/yanncauchepin/.local/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: typing-extensions in /home/yanncauchepin/.local/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (59.6.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: captum in /home/yanncauchepin/.local/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: matplotlib in /usr/lib/python3/dist-packages (from captum) (3.5.1)\n",
      "Requirement already satisfied: torch>=1.6 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from captum) (1.13.1)\n",
      "Requirement already satisfied: numpy in /home/yanncauchepin/.local/lib/python3.10/site-packages (from captum) (1.24.3)\n",
      "Requirement already satisfied: tqdm in /home/yanncauchepin/.local/lib/python3.10/site-packages (from captum) (4.64.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from torch>=1.6->captum) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from torch>=1.6->captum) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from torch>=1.6->captum) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from torch>=1.6->captum) (11.10.3.66)\n",
      "Requirement already satisfied: typing-extensions in /home/yanncauchepin/.local/lib/python3.10/site-packages (from torch>=1.6->captum) (4.9.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6->captum) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6->captum) (59.6.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25hDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mapie in /home/yanncauchepin/.local/lib/python3.10/site-packages (0.8.6)\n",
      "Requirement already satisfied: scipy in /home/yanncauchepin/.local/lib/python3.10/site-packages (from mapie) (1.11.1)\n",
      "Requirement already satisfied: scikit-learn in /home/yanncauchepin/.local/lib/python3.10/site-packages (from mapie) (1.2.0)\n",
      "Requirement already satisfied: packaging in /home/yanncauchepin/.local/lib/python3.10/site-packages (from mapie) (23.2)\n",
      "Requirement already satisfied: numpy>=1.21 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from mapie) (1.24.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from scikit-learn->mapie) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/yanncauchepin/.local/lib/python3.10/site-packages (from scikit-learn->mapie) (1.2.0)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/pip\", line 5, in <module>\n",
      "    from pip._internal.cli.main import main\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/cli/main.py\", line 9, in <module>\n",
      "    from pip._internal.cli.autocompletion import autocomplete\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
      "    from pip._internal.cli.main_parser import create_main_parser\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/cli/main_parser.py\", line 8, in <module>\n",
      "    from pip._internal.cli import cmdoptions\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/cli/cmdoptions.py\", line 23, in <module>\n",
      "    from pip._internal.cli.parser import ConfigOptionParser\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/cli/parser.py\", line 12, in <module>\n",
      "    from pip._internal.configuration import Configuration, ConfigurationError\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/configuration.py\", line 26, in <module>\n",
      "    from pip._internal.utils.logging import getLogger\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/utils/logging.py\", line 19, in <module>\n",
      "    from pip._vendor.rich.logging import RichHandler\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/rich/logging.py\", line 12, in <module>\n",
      "    from .traceback import Traceback\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/rich/traceback.py\", line 25, in <module>\n",
      "    from .syntax import Syntax\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/rich/syntax.py\", line 8, in <module>\n",
      "    from pip._vendor.pygments.lexer import Lexer\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/pygments/lexer.py\", line 48, in <module>\n",
      "    class Lexer(metaclass=LexerMeta):\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/pygments/lexer.py\", line 45, in __new__\n",
      "    return type.__new__(mcs, name, bases, d)\n",
      "KeyboardInterrupt\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/pip\", line 5, in <module>\n",
      "    from pip._internal.cli.main import main\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/cli/main.py\", line 9, in <module>\n",
      "    from pip._internal.cli.autocompletion import autocomplete\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
      "    from pip._internal.cli.main_parser import create_main_parser\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/cli/main_parser.py\", line 8, in <module>\n",
      "    from pip._internal.cli import cmdoptions\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/cli/cmdoptions.py\", line 23, in <module>\n",
      "    from pip._internal.cli.parser import ConfigOptionParser\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/cli/parser.py\", line 12, in <module>\n",
      "    from pip._internal.configuration import Configuration, ConfigurationError\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/configuration.py\", line 20, in <module>\n",
      "    from pip._internal.exceptions import (\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/exceptions.py\", line 13, in <module>\n",
      "    from pip._vendor.requests.models import Request, Response\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/requests/__init__.py\", line 136, in <module>\n",
      "    from . import packages\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/requests/packages.py\", line 8, in <module>\n",
      "    locals()[package] = __import__(vendored_package)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/idna/__init__.py\", line 2, in <module>\n",
      "    from .core import (\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/idna/core.py\", line 49, in <module>\n",
      "    def valid_label_length(label: Union[bytes, str]) -> bool:\n",
      "  File \"/usr/lib/python3.10/typing.py\", line 309, in inner\n",
      "    return cached(*args, **kwds)\n",
      "  File \"/usr/lib/python3.10/typing.py\", line 403, in __getitem__\n",
      "    return self._getitem(self, parameters)\n",
      "  File \"/usr/lib/python3.10/typing.py\", line 521, in Union\n",
      "    return _UnionGenericAlias(self, parameters)\n",
      "  File \"/usr/lib/python3.10/typing.py\", line 1019, in __init__\n",
      "    super().__init__(origin, inst=inst, name=name)\n",
      "  File \"/usr/lib/python3.10/typing.py\", line 950, in __init__\n",
      "    self.__origin__ = origin\n",
      "  File \"/usr/lib/python3.10/typing.py\", line 987, in __setattr__\n",
      "    if _is_dunder(attr) or attr in {'_name', '_inst', '_nparams',\n",
      "  File \"/usr/lib/python3.10/typing.py\", line 935, in _is_dunder\n",
      "    def _is_dunder(attr):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy\n",
    "!pip install tqdm\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install pyspark\n",
    "!pip install catboost\n",
    "!pip install shap\n",
    "!pip install pingouin\n",
    "!pip install seaborn\n",
    "!pip install torch\n",
    "!pip install captum\n",
    "!pip install sklearn\n",
    "!pip install mapie\n",
    "!pip install scikit-optimize\n",
    "!pip install pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b006539",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"/media/yanncauchepin/ExternalDisk/Datasets/MachineLearningTables/lending_club/LCDataDictionary.xlsx\"\n",
    "data_path = \"/media/yanncauchepin/ExternalDisk/Datasets/MachineLearningTables/lending_club/Loan_status_2007-2020Q3.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6939e91c",
   "metadata": {},
   "source": [
    "# Data Importation\n",
    "\n",
    "I initially plan to use **Pandas** librairy as I am used to work on few data, either in my current job or in my hands-on exercices. Since my computer freeze over many times, I decided to use **[Spark](https://spark.apache.org/)** library to manipulate this significative amount of data. This library involve map-reduce method to handle big data more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6c14c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "957f17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_excel(metadata_path, index_col=0)\n",
    "metadata = metadata.iloc[:-2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83ddf746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/23 14:57:44 WARN Utils: Your hostname, yanncauchepincomputer resolves to a loopback address: 127.0.1.1; using 192.168.43.208 instead (on interface wlp2s0)\n",
      "24/06/23 14:57:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/23 14:57:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/23 14:57:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yanncauchepin/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=57>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yanncauchepin/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/yanncauchepin/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yanncauchepin/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/yanncauchepin/.local/lib/python3.10/site-packages/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/home/yanncauchepin/.local/lib/python3.10/site-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/home/yanncauchepin/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/yanncauchepin/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/yanncauchepin/.local/lib/python3.10/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o13.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yanncauchepin/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/yanncauchepin/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LendingClubDataProcessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_spark = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "print(f\"Number of data: {df_spark.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1673bd",
   "metadata": {},
   "source": [
    "# Data Cleaning - Encoding Target\n",
    "\n",
    "Since the dataset was filled with metadata, I decided to check the consistency of the data to remove outer features. Additionnaly, I follow the instruction of removing *grade* and *sub_grade* features.\n",
    "\n",
    "Since I understood that the machine learning must classify whether a loan is *Fully Paid* or *Charged Off* specifically, I decided to encode the rest to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4751cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Metadata features: {len(metadata.index)}\")\n",
    "print(f\"Data features: {len(df_spark.columns)}\")\n",
    "\n",
    "outer_features = [feature for feature in df_spark.columns if feature not in metadata.index]\n",
    "\n",
    "print(f\"Unknown data features: {outer_features} ({len(outer_features)})\")\n",
    "df_spark = df_spark.drop(*outer_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8175f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = ['grade', 'sub_grade']\n",
    "df_spark = df_spark.drop(*features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3876cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df_spark.groupBy('loan_status').count()\n",
    "value_rate = value_counts.count() / df_spark.count()\n",
    "print(f\"Number of distincts values: {value_counts.count()} - {value_rate:.2e} %\")\n",
    "value_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mapping = {\n",
    "    'Fully Paid': 0,\n",
    "    'Charged Off': 1,\n",
    "    'Current': np.nan,\n",
    "    'Late (31-120 days)': np.nan,\n",
    "    'In Grace Period': np.nan,\n",
    "    'Late (16-30 days)': np.nan,\n",
    "    'Issued': np.nan,\n",
    "    'Does not meet the credit policy. Status:Fully Paid': np.nan,\n",
    "    'Does not meet the credit policy. Status:Charged Off': np.nan,\n",
    "    'Default': np.nan,\n",
    "    'Oct-2015': np.nan\n",
    "}\n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "df_spark = df_spark.withColumn(\"loan_status\", when(df_spark[\"loan_status\"] == \"Fully Paid\", 0)\n",
    "                   .when(df_spark[\"loan_status\"] == \"Charged Off\", 1)\n",
    "                   .otherwise(np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a37f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = [feature for feature in df_spark.columns if feature != 'loan_status']\n",
    "df_spark = df_spark.fillna({feature: \"nan\" if df_spark.schema[feature].dataType == 'string' else np.nan for feature in all_features})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205dda4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df_spark.groupBy('loan_status').count()\n",
    "value_rate = value_counts.count() / df_spark.count()\n",
    "print(f\"Number of distincts values: {value_counts.count()} - {value_rate:.2e} %\")\n",
    "value_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83528a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ced9754",
   "metadata": {},
   "source": [
    "# Data Reduction\n",
    "\n",
    "With the lack of time, and the complex structure of the table data, mixing types and nan, an advanced analysis would be interesting to pursue a modeling process. However, there is a interesting preprocess that consist to reduce the mix data into a low dimensional relevant data. In fact, this is common in large matrix of data where we can decompose it into a lower format which contains a significative explanative information. This is the case in mathematics with matrix decomposition, and an example of compressing files.\n",
    "\n",
    "Moreover, reducing the data handled in the modeling part could be interesting for daily professional users. Indeed, assuming the model perform well, it could be boring to insert all the data to assess a loan status. And all the data is not always available for daily uses.\n",
    "\n",
    "Here, a usefull process is to reduce the amount of features to the more relevant one by gradient boosting modeling. It is possible to extract the features importances of a tree model, whether it concerns classification or regression, and pursue our modeling work on the most important features. The question of the threshold is critical. Here I deciced to limit it by the selecting those until the cumulative exceed 90% of the total sum.\n",
    "\n",
    "In practice, the use of gradient boosting implementation depends on the data structure or on the underlying computed processes. Even if **[XGBoost](https://xgboost.readthedocs.io/en/stable/)** is widely \n",
    "used and usually represent the best models in Kaggle, others librairies exist. Here, I decided to use the library **[Catboost](https://catboost.ai/)** for it is inner preprocessing of categorical variables, which is not present in **XGBoost**. Since I have not enough time to encode all categorical features by myself, it is highly interesting. Of course, the evaluation of feature importances of this model will depends on the inner encoding process of **Catboost**, which will be certainly not reproduced in the next step. Additionnaly, I decided to not tune its hyperparameters or run it through several iterations considering the time allowed. The training was perform on small sample of the entire data to simplify this step and not demanding to much on my computer. I use the library **[Shap](https://shap.readthedocs.io/en/latest/index.html)** to get the features importances ; shapely additive explanations values are based on cooperative game theory.\n",
    "\n",
    "An interesting cross-validation would be the expertise of professionals who have intuitive knowledge. Obvsiously, the machine learning tools bring probably a better analysis. But, when we hesitate between two features, for example if two feature have a high correlation between them and a similar feature importance. My personal experience with physicians leads me to not exclude those opportunity of cross-validation. \n",
    "\n",
    "### Note\n",
    "\n",
    "If there is a issue when you run the script, even if the seed is set and must reproduce results, here is a script to run before removing unused features.\n",
    "\n",
    "```python\n",
    "selected_features = ['loan_amnt',\n",
    " 'funded_amnt',\n",
    " 'funded_amnt_inv',\n",
    " 'int_rate',\n",
    " 'fico_range_high',\n",
    " 'total_pymnt',\n",
    " 'total_pymnt_inv',\n",
    " 'total_rec_prncp',\n",
    " 'total_rec_int',\n",
    " 'recoveries',\n",
    " 'last_pymnt_amnt',\n",
    " 'last_fico_range_high',\n",
    " 'last_fico_range_low',\n",
    " 'mths_since_rcnt_il',\n",
    " 'mo_sin_old_rev_tl_op',\n",
    " 'last_credit_pull_d', \n",
    " 'last_pymnt_d']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442784f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gb = df_spark.sample(fraction=0.05, seed=1)\n",
    "print(f\"Number of data: {df_gb.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e255b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gb = df_gb.dropna(subset=['loan_status'])\n",
    "print(f\"Number of data: {df_gb.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1680e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_gb = df_gb.select(all_features)\n",
    "features_collected_gb = features_gb.collect()\n",
    "target_gb = df_gb.select('loan_status')\n",
    "target_collected_gb = target_gb.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d0f7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gb = np.array([list(feature) for feature in features_collected_gb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee730a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gb = np.array([feature['loan_status'] for feature in target_collected_gb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2387d119",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48cb9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a945bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [feature for (feature, dtype) in df_gb.dtypes if dtype=='string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6859dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostClassifier\n",
    "\n",
    "pool = Pool(data=X_gb, label=y_gb, feature_names=all_features, cat_features=categorical_features)\n",
    "\n",
    "catboost_model = CatBoostClassifier(iterations=100)\n",
    "catboost_model.fit(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc2cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "explainer = shap.Explainer(catboost_model)\n",
    "shap_values = explainer.shap_values(X_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8587e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_over_feature = np.sum(np.abs(shap_values), axis=0)\n",
    "feature_importance = pd.DataFrame(data=sum_over_feature, index=all_features, columns=['feature_importance'])\n",
    "feature_importance = feature_importance[feature_importance['feature_importance']>0]\n",
    "feature_importance = feature_importance.sort_values(by='feature_importance', ascending=False)\n",
    "feature_importance.shape\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cbd894",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance['cumulative_sum'] = feature_importance['feature_importance'].cumsum()\n",
    "total_sum = feature_importance['feature_importance'].sum()\n",
    "feature_importance['rate'] = feature_importance['cumulative_sum'] / total_sum\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdd2694",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.9\n",
    "selected_features = feature_importance[feature_importance['rate'] < threshold].index\n",
    "selected_features\n",
    "len(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b87fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = [feature for feature in all_features if feature not in selected_features]\n",
    "features_to_drop\n",
    "len(features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6d755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.drop(*features_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5422d2d0",
   "metadata": {},
   "source": [
    "# Data Cleaning - Encoding\n",
    "\n",
    "Now that 17 features are selected, it is time to clean them in a more appropriate way. To analyse the spark dataframe, the code below allows to highlight the cleaning need of each feature. I recommand you to run it before and after the following cleaning scripts if you would like to better understand it.\n",
    "\n",
    "### Imputation\n",
    "\n",
    "In a advanced version, I would impute nan with a more sophisticated process than by the mean of each feature. This could be executed by a modeling imputation with the use of machine learning models. Among many possibilities, it exists k-nearest neighbors imputations or deep learning-based imputations. Nevertheless, we can observe than there is only few data containing nan so it is not so critical for a quick hands-on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaa72bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import isnan\n",
    "# for feature, dtype in df_spark.dtypes:\n",
    "#     print(\"====================================\")\n",
    "#     print(f\"FEATURE: {feature}\")\n",
    "#     if dtype=='string':\n",
    "#         value_counts = df_spark.groupBy(feature).count()\n",
    "#         value_rate = value_counts.count() / df_spark.count()\n",
    "#         print(f\"Number of distincts values: {value_counts.count()} - {value_rate:.2e} %\")\n",
    "#         value_counts.show()\n",
    "#     nan_count = df_spark.filter(df_spark[feature].isNull() | isnan(df_spark[feature])).count()\n",
    "#     nan_rate = nan_count / df_spark.count()\n",
    "#     print(f\"{nan_count} NaN - {nan_rate:.2e} %\")\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e36c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"====================================\")\n",
    "feature = 'last_pymnt_d'\n",
    "print(f\"FEATURE: {feature}\")\n",
    "value_counts = df_spark.groupBy(feature).count()\n",
    "value_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbace96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "to_float_udf = udf(to_float, FloatType())\n",
    "df_spark = df_spark.withColumn(\"last_fico_range_high\", to_float_udf(df_spark[\"last_fico_range_high\"]))\n",
    "df_spark = df_spark.withColumn(\"last_pymnt_amnt\", to_float_udf(df_spark[\"last_pymnt_amnt\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de72519",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.createOrReplaceTempView(\"lending_club_1\")\n",
    "sql_expression = \"\"\"\n",
    "CASE\n",
    "    WHEN last_pymnt_d LIKE 'Jan-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 0/12\n",
    "    WHEN last_pymnt_d LIKE 'Feb-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 1/12\n",
    "    WHEN last_pymnt_d LIKE 'Mar-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 2/12\n",
    "    WHEN last_pymnt_d LIKE 'Apr-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 3/12\n",
    "    WHEN last_pymnt_d LIKE 'May-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 4/12\n",
    "    WHEN last_pymnt_d LIKE 'Jun-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 5/12\n",
    "    WHEN last_pymnt_d LIKE 'Jul-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 6/12\n",
    "    WHEN last_pymnt_d LIKE 'Aug-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 7/12\n",
    "    WHEN last_pymnt_d LIKE 'Sep-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 8/12\n",
    "    WHEN last_pymnt_d LIKE 'Oct-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 9/12\n",
    "    WHEN last_pymnt_d LIKE 'Nov-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 10/12\n",
    "    WHEN last_pymnt_d LIKE 'Dec-%' THEN CAST(SUBSTRING(last_pymnt_d, 5) AS FLOAT) + 11/12\n",
    "    ELSE NULL\n",
    "END AS last_pymnt_d_num\n",
    "\"\"\"\n",
    "df_spark = spark.sql(f\"\"\"\n",
    "SELECT *, {sql_expression}\n",
    "FROM lending_club_1\n",
    "\"\"\")\n",
    "\n",
    "df_spark = df_spark.drop(\"last_pymnt_d\")\n",
    "df_spark = df_spark.withColumnRenamed('last_pymnt_d_num', 'last_pymnt_d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a41c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.createOrReplaceTempView(\"lending_club_2\")\n",
    "sql_expression = \"\"\"\n",
    "CASE\n",
    "    WHEN last_credit_pull_d LIKE 'Jan-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 0/12\n",
    "    WHEN last_credit_pull_d LIKE 'Feb-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 1/12\n",
    "    WHEN last_credit_pull_d LIKE 'Mar-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 2/12\n",
    "    WHEN last_credit_pull_d LIKE 'Apr-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 3/12\n",
    "    WHEN last_credit_pull_d LIKE 'May-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 4/12\n",
    "    WHEN last_credit_pull_d LIKE 'Jun-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 5/12\n",
    "    WHEN last_credit_pull_d LIKE 'Jul-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 6/12\n",
    "    WHEN last_credit_pull_d LIKE 'Aug-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 7/12\n",
    "    WHEN last_credit_pull_d LIKE 'Sep-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 8/12\n",
    "    WHEN last_credit_pull_d LIKE 'Oct-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 9/12\n",
    "    WHEN last_credit_pull_d LIKE 'Nov-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 10/12\n",
    "    WHEN last_credit_pull_d LIKE 'Dec-%' THEN CAST(SUBSTRING(last_credit_pull_d, 5) AS FLOAT) + 11/12\n",
    "    ELSE NULL\n",
    "END AS last_credit_pull_d_num\n",
    "\"\"\"\n",
    "df_spark = spark.sql(f\"\"\"\n",
    "SELECT *, {sql_expression}\n",
    "FROM lending_club_2\n",
    "\"\"\")\n",
    "\n",
    "df_spark = df_spark.drop(\"last_credit_pull_d\")\n",
    "df_spark = df_spark.withColumnRenamed('last_credit_pull_d_num', 'last_credit_pull_d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a3645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_float_rate = udf(lambda x: float(x.replace('%', '')) / 100, FloatType())\n",
    "df_spark = df_spark.withColumn('int_rate', convert_to_float_rate(df_spark['int_rate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab30efbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "numerical_selected_features = [feature for (feature, dtype) in df_spark.dtypes if (dtype!='string' and feature !='loan_status')]\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=numerical_selected_features,\n",
    "    outputCols=numerical_selected_features\n",
    ").setStrategy(\"mean\")\n",
    "\n",
    "df_spark = imputer.fit(df_spark).transform(df_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3efbe0",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "### Correlation\n",
    "\n",
    "It is obvioulsy interesting to analysis correlation, or even partial correlation, between features to identify whether some selected features could be additionnaly removed. Indeed, there could be an issue while modeling colinear features, which could be observed by a brute correlation equals to 1.\n",
    "\n",
    "Moreover, it is interesting to evaluate the partial correlations by removing the covariance on the different features. It allows to extract the direct relationship between each pair of features. A highly partial correlation could be the origin of an unnecessary dimension modeling. Therefore, the data reduction could be also improved here by removing features included in pair of partial correlation with high values, and which have the lowest features importances, i.e. provided here by gradient boosting. The question of the threshold is still critical here, as it was in the data reduction. \n",
    "\n",
    "\n",
    "### Distribution over the *loan_status* target\n",
    "\n",
    "An other aspect of unrelevant features could be the significative difference between the distribution of features among all the modalities of the target *loan_status*. Here I plot the boxplot of each feature whether it concerns the *Charged Off* or *Fully Paid*, and I run a statistical test to evaluate the null hypothesis that there is a significative difference observing the means of the distinct distribtutions. Sometimes, graphical displays do not match with the statistical test.\n",
    "\n",
    "### Note\n",
    "\n",
    "I have honestyly prioritized the modeling section to return a productive work. This section was therefore performed at the end of the hands-on. So the conclusion it provides will not be applied in the rest of this work. Considering the observation of this section, it would be interesting to evaluate the relevance of considering or not each debatable features in the modeling part. \n",
    "\n",
    "But keep in mind this analysis was processed on a small set from the entire data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f79120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_spark.sample(fraction=0.005, seed=1)\n",
    "print(f\"Number of data: {df.count()}\")\n",
    "df_labeled = df.dropna(subset=['loan_status'])\n",
    "print(f\"Number of labeled data: {df_labeled.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ac9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [feature for feature in list(df_spark.columns) if feature != 'loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eb7743",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_collected = df_labeled.select(features).collect()\n",
    "target_collected = df_labeled.select('loan_status').collect()\n",
    "X_labeled = np.array([list(feature) for feature in features_collected], dtype=float)\n",
    "y_labeled = np.array([feature['loan_status'] for feature in target_collected], dtype=float)\n",
    "y_labeled = y_labeled.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d39536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c30c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.DataFrame(X_labeled.astype(float), columns=features)\n",
    "df_combined['loan_status'] = y_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba5d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = 5\n",
    "num_rows = (len(features) + num_cols - 1) // num_cols\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 4))\n",
    "axes = axes.flatten()\n",
    "for i, feature in enumerate(df_combined.columns):\n",
    "    sns.boxplot(df_combined[feature], ax=axes[i])\n",
    "    axes[i].set_title(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ecfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.pairplot(df_combined[features])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c5a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_combined.corr(numeric_only=False).round(2)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0, cbar_kws={'label': 'Partial Correlation'})\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "pcorr_matrix = df_combined.pcorr().round(2)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(pcorr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0, cbar_kws={'label': 'Partial Correlation'})\n",
    "plt.title('Partial Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5322b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = 6\n",
    "num_rows = 6\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 4))\n",
    "axes = axes.flatten()\n",
    "for i, feature in enumerate(features):\n",
    "    df_fully_paid = df_combined[df_combined['loan_status'] == 0.0][feature]\n",
    "    df_charged_off = df_combined[df_combined['loan_status'] == 1.0][feature]\n",
    "    sns.boxplot(df_fully_paid.tolist(), ax=axes[2*i], color='C1')\n",
    "    axes[2*i].set_title(f\"Fully Paid\\n{feature}\")\n",
    "    sns.boxplot(df_charged_off.tolist(), ax=axes[2*i+1], color='C2')\n",
    "    axes[2*i+1].set_title(f\"Charged Off\\n{feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9137add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, f_oneway\n",
    "\n",
    "fully_paid = df_combined[df_combined['loan_status'] == 0][features]\n",
    "charged_off = df_combined[df_combined['loan_status'] == 1][features]\n",
    "t_stat, p_val = ttest_ind(fully_paid, charged_off)\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    print(f\"Feature: {feature}\")\n",
    "    print(f\"\\tT-statistic: {t_stat[i]}\")\n",
    "    print(f\"\\tP-value: {p_val[i]}\")\n",
    "    if p_val[i] < 0.05:\n",
    "        print(\"\\tReject the null hyptothesis ~ Significant difference between the means of binary 'loan_status'\")\n",
    "    else:\n",
    "        print(\"\\tFail to reject the null hyptothesis ~ No significant difference between the means of binary 'loan_status'\")\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b745349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan\n",
    "df_nan = df.filter(isnan(df['loan_status']))\n",
    "print(f\"Number of NaN data: {df_nan.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_collected = df_nan.select(features).collect()\n",
    "X_unlabeled = np.array([list(feature) for feature in features_collected], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b383f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_ = np.unique(y_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68caadf",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "When it comes to choose a machine learnning classifier to predict *loan_status*, two main classes are well known to perform good evaluation: deep neural networks and gradient boosting. *XGBoost* is one of the best model in common Kaggle competition. Several articles compare their performances:\n",
    "\n",
    "**Article**: [Why do tree-based models still outperform deep learning on typical tabular data?](https://proceedings.neurips.cc/paper_files/paper/2022/hash/0378c7692da36807bdec87ab043cdadc-Abstract-Datasets_and_Benchmarks.html)\n",
    "\n",
    "**Article**: [Credit Risk Assessment based on Gradient Boosting Decision Tree](https://www.sciencedirect.com/science/article/pii/S1877050920315842)\n",
    "\n",
    "**Article**: [Deep Learning vs. Gradient Boosting: Benchmarking state-of-the-art machine learning algorithms for credit scoring](https://arxiv.org/abs/2205.10535)\n",
    "\n",
    "However, to illustrate my abilities in this hands-on, I choose to apply a deep multilayer perceptron network to classify loans since I already use a gradient boosting model for data reduction. Initially, since the goal of this work is to target only two *loan_status* among a dozen, I compare it to spam filtered in Google. Even if it has evolved, I keep in mind that the underlying process is related to semi-supervised classification. This involves unlabeled and labeled data. \n",
    "\n",
    "Additionnaly, even if I have already work on TensorFlow and the high level Keras framework, I choose to apply Pytorch to gain some times since I am habit to work on it in my current schedule. I have added several elements to its model to bring a little more complexity.\n",
    "\n",
    "### Scalability\n",
    "\n",
    "The rate of the train set could be filled when initializing the model. Nevertheless, I recommand to use between 30% and 15% of the entire data to keep a reliable test set to evaluate properly the model. Here, the amount of data is too large for a quick hands-on. You can see that I have also test my model on larger sample of the spark dataframe than the training sample, and the evaluation still pretty nice. If I have more time, I would probably try to take a very larger size of data. \n",
    "\n",
    "### Conformal predicted intervals\n",
    "\n",
    "**[Mapie](https://mapie.readthedocs.io/en/latest/)** library provides conformal predicted intervals, whether it concerns regression or classification. However, it could be only applied on **Scikit-learn** library format ; explaining why I must to implement a intermediate class named **MLPEstimatorSklearn**. Unfortunaly, I still have a issue while I deliver this hands-on: while *fordward_proba* effectively provides a float, the inner process of **MapieClassifier** probably translate it into a boolean since float values could be extremely near from both 0 and 1. And I suppose their is a process that translate 0 and 1 to boolean, like in C language. Something like this, I did not deepen this issue.\n",
    "\n",
    "**Article**: [Conformal Prediction Sets for Ordinal Classification](https://proceedings.neurips.cc/paper_files/paper/2023/hash/029f699912bf3db747fe110948cc6169-Abstract-Conference.html)\n",
    "\n",
    "**Article**: [Conjugate Conformal Prediction for Online Binary Classification](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=77fbaec8faa58fc547de29082538b16c328545df)\n",
    "\n",
    "### Hyperparameters tuning\n",
    "\n",
    "Inner hyperparameters tuning whether it is by calling randomized or bayes optimization. I fill a range for each hyperparameter which could be debatable. I have set string in hidden_layer_size hyperparameter which represent the hidden architecture of the MLP classifier since bayes search could not accept a tuple of tuple for its range, even with extra functions from **Skopt** such as *Optional*. This is why I handle it through tuple in the low-level implementation of **MLP**. Unfornutalely, I also have an issue with bayes search. While randomized search work, bayesian tuning seems to be not up to date to **Numpy** library. And I was not able to find a quick solution within the time allowed. If you want to optimize hyperparamaters of the model, please run the method *randomized_search* before fitting the model. Note also that it could not explore the initial hyperparameters settings, which is already highly interesting. Nevertheless, it is also highly possible to find a better architecture, for example, with enough iterations of search. Even if the exploration is limited here, as a reminder, there is not a suitable architecture for any problems since there is not prior strategy to find the optimal architecture.\n",
    "\n",
    "### Interprating method through integrated gradients\n",
    "\n",
    "Deep neural networks are advanced black-box processes which are difficult to interpret. However, it exists some methods to bring interpration on their well performance. One of the most interesting one is integrated gradients since it checks many axioms. By interpolating gradients, it is possible to identify which features is the most important for the attribution of a deep neural netwoks, whether it concerns classification or regression for example, even in NLP, Computer Vision fields. Here I use **[Captum](https://captum.ai/)** libraby which is linked to **Pytorch** to accelerate its computing. It involves NVIDIA libraries to parallel computing, explaining the quite large size of the docker image of my trained MLP binary classifier. I do not recommend to run *compute_integreated_gradients*  on a large amount to not freeze your computer, which was my case. Nevertheless, it allows to provides the importances of each feature toward the attribution of the *loan_status* attribution. I decided to normalized it in min max standardization to better catch the importances, while the original value is extremely small.\n",
    "\n",
    "**Article** : [Axiomatic Attribution for Deep Networks](http://proceedings.mlr.press/v70/sundararajan17a.html)\n",
    "\n",
    "\n",
    "However, Kolmogorov Arnold networks are a well alternative to black-box neural networks. Without going deepen to desrible the KAN models since it is not the subject of its hands-on, it computes activation functions by the use of splines, instead of computing weights and biaises. It could be very interesting if you would like to get a \"generic\" function, linking intputs to outputs. It generally needs a smaller architecture to provides same evaluation than a black-box neural networks. A main interest coulb be explain when physician would like to identify a generic function of a unknown process, for example.\n",
    "\n",
    "\n",
    "### Note\n",
    "\n",
    "I have tried different strategies to check the performance of the model. Even if I know it is better to target a single neuron in the output layer in case of binary classifier, I also tried to used two neurons. Why I did this? Because I hesitated on the well approach to return probabilities of the two class. I know that softmax activation function is the activation function to apply to return probabilities for each neuron in a multi-class classifier. While sigmoid function also returns value between 0 and 1, I honestly hesitated whether or not is it appropriate to return the probability of the class 1, i.e. *Charged Off* here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b5568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from captum.attr import IntegratedGradients\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from mapie.classification import MapieClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from skopt import BayesSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layer_sizes, activation_name, p_dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        if isinstance(self.hidden_layer_sizes, str):\n",
    "            self.hidden_layer_sizes = eval(self.hidden_layer_sizes)\n",
    "        self.activation_name = activation_name\n",
    "        self.p_dropout = p_dropout\n",
    "        if activation_name == \"Relu\":\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation_name == \"Sigmoid\":\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation_name == \"Softmax\":\n",
    "            self.activation = nn.Softmax(dim=1)\n",
    "        elif activation_name == \"Tanh\":\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation_name == \"Leaky_relu\":\n",
    "            self.activation = nn.LeakyReLU()\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported activation: {self.activation_name}')\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(self.input_size, self.hidden_layer_sizes[0]))\n",
    "        layers.append(self.activation)\n",
    "        for i in range(len(self.hidden_layer_sizes) - 1):\n",
    "            layers.append(nn.Linear(self.hidden_layer_sizes[i], self.hidden_layer_sizes[i + 1]))\n",
    "            layers.append(self.activation)\n",
    "            layers.append(nn.Dropout(p=self.p_dropout))\n",
    "        layers.append(nn.Linear(self.hidden_layer_sizes[-1], self.output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward_proba(self, X):\n",
    "        output = self.model(X)\n",
    "        output = self.sigmoid(output)\n",
    "        return output.float()\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        output = self.model(X)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class MLPEstimatorSklearn():\n",
    "    def __init__(self, **params):\n",
    "        self.input_size = params.get(\"input_size\")\n",
    "        self.output_size = params.get(\"output_size\")\n",
    "        self.hidden_layer_sizes = params.get(\"hidden_layer_sizes\", (60, 60))\n",
    "        self.activation_name = params.get(\"activation_name\", \"Relu\")\n",
    "        self.loss = params.get(\"loss\", \"binary_cross_entropy\")\n",
    "        self.optimizer_name = params.get(\"optimizer_name\", \"Adam\")\n",
    "        self.learning_rate = params.get(\"learning_rate\", 1e-3)\n",
    "        self.batch_size = params.get(\"batch_size\", 50)\n",
    "        self.weight_decay = params.get(\"weight_decay\", 0)\n",
    "        self.p_dropout = params.get(\"p_dropout\", 0.2)\n",
    "        self.early_stopping = params.get(\"early_stopping\", True)\n",
    "        self.epochs = params.get(\"epochs\", 200)\n",
    "        self.patience = params.get(\"patience\", 10)\n",
    "        self.verbose = params.get(\"verbose\", True)\n",
    "        self.classes_ = classes_\n",
    "\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = MLP(self.input_size, self.output_size, self.hidden_layer_sizes, self.activation_name, self.p_dropout).to(self.device)\n",
    "\n",
    "        if self.loss == \"binary_cross_entropy\":\n",
    "            self.criterion = nn.BCELoss()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss: {self.loss}\")\n",
    "\n",
    "        if self.optimizer_name == \"SGD\":\n",
    "            self.optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate, momentum=0.9, weight_decay=self.weight_decay)\n",
    "        elif self.optimizer_name == \"Adam\":\n",
    "            self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer: {self.optimizer}\")\n",
    "            \n",
    "    def next_batch(self, inputs, targets, batchSize):\n",
    "        inputs_tensor = torch.from_numpy(inputs).float()\n",
    "        targets_tensor = torch.from_numpy(targets).float().unsqueeze(1)\n",
    "        for i in range(0, inputs_tensor.shape[0], batchSize):\n",
    "            yield (inputs_tensor[i:i + batchSize], targets_tensor[i:i + batchSize])\n",
    "\n",
    "    def augment_data(self, X_unlabeled, noise_level=0.1):\n",
    "        noise = noise_level * torch.randn_like(X_unlabeled)\n",
    "        return X_unlabeled + noise\n",
    "            \n",
    "    def fit(self, X, y, X_unlabeled=None):\n",
    "        self.classes_ = np.unique(y)\n",
    "        running_losses_1 = list()\n",
    "        if self.early_stopping:\n",
    "            best_loss = float('inf')\n",
    "            count = 0\n",
    "        if self.verbose:\n",
    "            epoch_iterator_1 = tqdm(range(self.epochs), desc=\"Supervised training ; epochs\", unit=\"epoch\")\n",
    "        else:\n",
    "            epoch_iterator_1 = range(self.epochs)\n",
    "        for epoch in epoch_iterator_1:\n",
    "            samples = 0\n",
    "            train_loss = 0.0\n",
    "            self.model.train(True)\n",
    "            for i, (batchX, batchY) in enumerate(self.next_batch(X, y, self.batch_size)):\n",
    "                batchX = batchX.to(self.device)\n",
    "                batchY = batchY.to(self.device)\n",
    "                batchY.requires_grad = True\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(batchX)\n",
    "                loss = self.criterion(outputs, batchY)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                samples += batchY.size(0)\n",
    "            running_loss = train_loss / samples\n",
    "            running_losses_1.append(running_loss)\n",
    "            if self.verbose:\n",
    "                epoch_iterator_1.set_postfix(train_loss=running_loss)\n",
    "            if self.early_stopping:\n",
    "                if running_loss < best_loss:\n",
    "                    best_loss = running_loss\n",
    "                    count = 0\n",
    "                else:\n",
    "                    count += 1\n",
    "                if count >= self.patience:\n",
    "                    break\n",
    "        if self.verbose:\n",
    "            epoch_iterator_1.close()\n",
    "        self.running_losses_1 = [loss for loss in running_losses_1 if loss <= best_loss]\n",
    "        if X_unlabeled is not None:\n",
    "            best_loss = float('inf')\n",
    "            running_losses_2 = list()\n",
    "            X_unlabeled = torch.from_numpy(X_unlabeled).float()\n",
    "            augmented_X_unlabeled = self.augment_data(X_unlabeled)\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                pseudo_labels = self.model(X_unlabeled)\n",
    "                pseudo_labels = (pseudo_labels > 0.5).float().squeeze()\n",
    "            \n",
    "            X_combined = torch.cat((torch.from_numpy(X).float(), augmented_X_unlabeled.cpu()), 0).to(self.device)\n",
    "            y_combined = torch.cat((torch.from_numpy(y).float().squeeze(), pseudo_labels), 0).to(self.device)\n",
    "\n",
    "            if self.verbose:\n",
    "                epoch_iterator_2 = tqdm(range(self.epochs), desc=\"Semi-supervised training ; epochs\", unit=\"epoch\")\n",
    "            else:\n",
    "                epoch_iterator_2 = range(self.epochs)\n",
    "            for epoch in epoch_iterator_2:\n",
    "                samples = 0\n",
    "                train_loss = 0.0\n",
    "                self.model.train(True)\n",
    "                dataset = TensorDataset(X_combined, y_combined)\n",
    "                for i, (batchX, batchY) in enumerate(self.next_batch(X, y, self.batch_size)):\n",
    "                    batchX = batchX.to(self.device)\n",
    "                    batchY = batchY.to(self.device)\n",
    "                    batchY.requires_grad = True\n",
    "                    self.optimizer.zero_grad()\n",
    "                    outputs = self.model(batchX)\n",
    "                    loss = self.criterion(outputs, batchY)\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    train_loss += loss.item()\n",
    "                    samples += batchY.size(0)\n",
    "                running_loss = train_loss / samples\n",
    "                running_losses_2.append(running_loss)\n",
    "                if self.verbose:\n",
    "                    epoch_iterator_2.set_postfix(train_loss=running_loss)\n",
    "                if self.early_stopping:\n",
    "                    if running_loss < best_loss:\n",
    "                        best_loss = running_loss\n",
    "                        count = 0\n",
    "                    else:\n",
    "                        count += 1\n",
    "                    if count >= self.patience:\n",
    "                        break\n",
    "            if self.verbose:\n",
    "                epoch_iterator_2.close()\n",
    "            self.running_losses_2 = [loss for loss in running_losses_2 if loss <= best_loss]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        X = torch.from_numpy(X).float().to(self.device)\n",
    "        y_pred = self.model.forward(X)\n",
    "        if self.device == \"cpu\":\n",
    "            y_pred = y_pred.cpu().detach().numpy()\n",
    "        else:\n",
    "            y_pred = y_pred.detach().numpy()\n",
    "        return (y_pred > 0.5).astype(int)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        self.model.eval()\n",
    "        X = torch.from_numpy(X).float().to(self.device)\n",
    "        y_proba = self.model.forward_proba(X)\n",
    "        if self.device == \"cpu\":\n",
    "            y_proba = y_proba.cpu().detach().numpy().astype(float)\n",
    "        else:\n",
    "            y_proba = y_proba.detach().numpy().astype(float)\n",
    "        y_proba = y_proba.squeeze().astype(np.float32)\n",
    "        return np.column_stack((1 - y_proba, y_proba))\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        params = {\n",
    "            \"input_size\": self.input_size,\n",
    "            \"output_size\": self.output_size,\n",
    "            \"hidden_layer_sizes\": self.hidden_layer_sizes,\n",
    "            \"activation_name\": self.activation_name,\n",
    "            \"loss\": self.loss,\n",
    "            \"optimizer_name\": self.optimizer_name,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"weight_decay\": self.weight_decay,\n",
    "            \"p_dropout\": self.p_dropout,\n",
    "            \"early_stopping\": self.early_stopping,\n",
    "            \"epochs\": self.epochs,\n",
    "            \"patience\": self.patience,\n",
    "            \"verbose\": self.verbose\n",
    "        }\n",
    "        return params\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            if hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "    def get_mlp(self):\n",
    "        return self.model\n",
    "    \n",
    "class MLPBinaryClassifier():\n",
    "    def __init__(self, X, y, split_test, X_unlabeled=None, **params):\n",
    "        self.model = MLPEstimatorSklearn(**params)\n",
    "        self.X = X\n",
    "        self.X_unlabeled = X_unlabeled\n",
    "        self.y = y\n",
    "        \n",
    "        self.y = MLPBinaryClassifier.float_to_class(self.y).ravel()\n",
    "        \n",
    "        self.split_test = split_test\n",
    "        self.split_data()\n",
    "        \n",
    "        self.standardize(self.X_train_cal)\n",
    "        self.X_train_standard = self.standardize_X(self.X_train)\n",
    "        self.X_cal_standard = self.standardize_X(self.X_cal)\n",
    "        if isinstance(self.X_unlabeled, np.ndarray):\n",
    "            self.X_unlabeled_standard = self.standardize_X(self.X_unlabeled)\n",
    "        else :\n",
    "            self.X_unlabeled_standard = None\n",
    "        self.y_train_standard = self.y_train\n",
    "        self.y_cal_standard = self.y_cal.reshape(-1,1)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def float_to_class(y):\n",
    "        threshold = 0.5\n",
    "        return (y >= threshold).astype(int)\n",
    "    \n",
    "    def split_data(self):\n",
    "        self.X_train_cal, self.X_test, self.y_train_cal, self.y_test = train_test_split(\n",
    "            self.X, self.y, test_size=self.split_test, shuffle=True, random_state=1, stratify=self.y)\n",
    "        self.X_train, self.X_cal, self.y_train, self.y_cal = train_test_split(\n",
    "            self.X_train_cal, self.y_train_cal, test_size=0.25, shuffle=True, random_state=1, stratify=self.y_train_cal)\n",
    "\n",
    "    def standardize(self, X):\n",
    "        self.scaler_X_train = StandardScaler()\n",
    "        self.scaler_X_train.fit(X)         \n",
    "\n",
    "\n",
    "    def standardize_X(self, X):\n",
    "        X_new = self.scaler_X_train.transform(X)\n",
    "        return X_new\n",
    "    \n",
    "\n",
    "\n",
    "    def bayes_search(self, param_bayes, n_iter, n_points=1, cv=5, scoring='accuracy',\n",
    "                 verbose=3, n_jobs=1) :\n",
    "        cv = StratifiedKFold(n_splits=cv, shuffle=True, random_state=1)\n",
    "        bayes_search = BayesSearchCV(self.model, param_bayes, n_iter=n_iter,\n",
    "                                     n_points=n_points, cv=cv, scoring=scoring,\n",
    "                                     verbose=verbose, return_train_score=True,\n",
    "                                     n_jobs=n_jobs, random_state=1)\n",
    "        bayes_search.fit(self.X_train_standard, self.y_train_standard)\n",
    "        results_df = pd.DataFrame(bayes_search.cv_results_)\n",
    "        self.model = bayes_search.best_estimator_\n",
    "        print(f'Best hyperparameters bayes search : {bayes_search.best_params_}')\n",
    "        return results_df\n",
    "\n",
    "    def randomized_search(self, param_randomized, n_iter, cv=5, scoring='accuracy',\n",
    "                      verbose=3, n_jobs=1) :\n",
    "        cv = StratifiedKFold(n_splits=cv, shuffle=True, random_state=1)\n",
    "        randomized_search = RandomizedSearchCV(self.model, param_randomized,\n",
    "                                               n_iter=n_iter, cv=cv, scoring=scoring,\n",
    "                                               verbose=verbose, return_train_score=True,\n",
    "                                               n_jobs=n_jobs, random_state=1)\n",
    "        randomized_search.fit(self.X_train_standard, self.y_train_standard)\n",
    "        results_df = pd.DataFrame(randomized_search.cv_results_)\n",
    "        self.model = randomized_search.best_estimator_\n",
    "        print(f'Best hyperparameters randomized search : {randomized_search.best_params_}')\n",
    "        return results_df\n",
    "\n",
    "    def fit(self, method=\"lac\"):\n",
    "        self.model.fit(self.X_train_standard, self.y_train_standard, self.X_unlabeled_standard)\n",
    "        self.model_mapie = MapieClassifier(estimator=self.model, cv=\"prefit\", method=method)\n",
    "        self.model_mapie.fit(self.X_cal_standard, self.y_cal_standard)\n",
    "\n",
    "    def predict(self, X, alpha=0.05):\n",
    "        X_standard = self.standardize_X(X)\n",
    "        y_pred, y_ps = self.model_mapie.predict(X_standard, alpha=alpha)\n",
    "        return y_pred, y_ps\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_metrics(metric, y_true, y_pred):\n",
    "        y_pred = MLPBinaryClassifier.float_to_class(y_pred)\n",
    "        accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "        precision = metrics.precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        recall = metrics.recall_score(y_true, y_pred, average='weighted')\n",
    "        f1 = metrics.f1_score(y_true, y_pred, average='weighted')\n",
    "        metrics_dict = {\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1': f1,\n",
    "        }\n",
    "        if metric != 'all':\n",
    "            metrics_dict = {metric: metrics_dict[metric]}\n",
    "        return metrics_dict\n",
    "\n",
    "\n",
    "    def model_performance(self, metric='all'):\n",
    "        y_pred_train, _ = self.predict(self.X_train)\n",
    "        scores_train = MLPBinaryClassifier.compute_metrics(metric, self.y_train, y_pred_train)\n",
    "        y_pred_test, _ = self.predict(self.X_test)\n",
    "        scores_test = MLPBinaryClassifier.compute_metrics(metric, self.y_test, y_pred_test)\n",
    "        data = {}\n",
    "        for key, value in scores_train.items():\n",
    "            data['Train Set - '+key] = [value]\n",
    "        for key, value in scores_test.items():\n",
    "            data['Test Set - '+key] = [value]\n",
    "        df_scores = pd.DataFrame(data=data).T\n",
    "        df_scores.columns = ['Scores']\n",
    "        return df_scores\n",
    "\n",
    "    def model_performance_test(self, X_test, y_test, metric='all'):\n",
    "        y_pred_test, _ = self.predict(X_test)\n",
    "        scores_test = MLPBinaryClassifier.compute_metrics(metric, y_test, y_pred_test)\n",
    "        data = {}\n",
    "        for key, value in scores_test.items():\n",
    "            data['Test Set - '+key] = [value]\n",
    "        df_scores = pd.DataFrame(data=data).T\n",
    "        df_scores.columns = ['Scores']\n",
    "        return df_scores\n",
    "\n",
    "    def receiver_operating_characteristics(self):\n",
    "        y_pred_test, _ = self.predict(self.X_test)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(self.y_test, y_pred_test)\n",
    "        plt.plot(fpr, tpr)\n",
    "        plt.title(\"Receiver Operating Characteristics\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.show()\n",
    "\n",
    "    def compute_integrated_gradients(self, X, baseline=None, steps=50):\n",
    "        def preprocess_input(X):\n",
    "            return torch.tensor(X, dtype=torch.float32)\n",
    "        input_tensor = preprocess_input(X)\n",
    "        if baseline is None:\n",
    "            baseline = torch.zeros_like(input_tensor)\n",
    "        integrated_gradients = IntegratedGradients(self.model.get_mlp())\n",
    "        attributions = integrated_gradients.attribute(input_tensor, baseline, target=0, n_steps=steps)\n",
    "        attributions_df = pd.DataFrame(attributions.cpu().detach().numpy(), columns=features)\n",
    "        avg_attributions = attributions_df.mean(axis=0)\n",
    "        avg_abs_attributions = avg_attributions.abs()\n",
    "        def custom_minmax_scaler(data, feature_range=(0, 100)):\n",
    "            min_val = np.min(data)\n",
    "            max_val = np.max(data)\n",
    "            if max_val - min_val == 0:\n",
    "                return np.zeros_like(data) if feature_range[0] == 0 else np.full_like(data, feature_range[0])\n",
    "            scale = (feature_range[1] - feature_range[0]) / (max_val - min_val)\n",
    "            min_range = feature_range[0]\n",
    "            scaled_data = scale * (data - min_val) + min_range\n",
    "            return scaled_data\n",
    "        normalized_data = custom_minmax_scaler(avg_abs_attributions.values.reshape(-1, 1)).astype(float)\n",
    "        np.set_printoptions(suppress=True, precision=2)\n",
    "        normalized_attributions = pd.DataFrame(normalized_data, columns=['attribution'], index=features)\n",
    "        sorted_attributions = normalized_attributions.sort_values(by=\"attribution\", ascending=False)\n",
    "        return sorted_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab236126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import loguniform, uniform\n",
    "from skopt.space import Real\n",
    "\n",
    "params = {    \n",
    "    \"init\" : {\n",
    "        \"input_size\" : len(features),\n",
    "        \"output_size\" : 1,\n",
    "        \"hidden_layer_sizes\" : (60,60),\n",
    "        \"activation_name\" : \"Relu\",\n",
    "        \"optimizer_name\" : \"Adam\",\n",
    "        \"learning_rate\" : 1e-3,\n",
    "        \"batch_size\" : 50,\n",
    "        \"weight_decay\" : 0,\n",
    "        \"p_dropout\" : 0.3,\n",
    "        \"loss\" : \"binary_cross_entropy\",\n",
    "        \"early_stopping\" : True,\n",
    "        \"epochs\" : 200,\n",
    "        \"patience\" : 10,\n",
    "        \"verbose\" : True\n",
    "    },\n",
    "    \"randomized\": {\n",
    "        \"hidden_layer_sizes\" : [(10,),(50,),(100,),(10,10),(50,50),(60,60),(100,50),(100,100),(100,50,25)],\n",
    "        \"activation_name\" :  [\"Relu\", \"Sigmoid\", \"Tanh\", \"Leaky_relu\", \"Softmax\"],\n",
    "        \"learning_rate\" : loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\" : list(np.arange(10,500, 10)),\n",
    "        \"optimizer_name\" : [\"Adam\", \"SGD\"],\n",
    "        \"alpha\" : np.logspace(-3,0,19),\n",
    "        \"weight_decay\" : loguniform(1e-5, 1),\n",
    "        \"p_dropout\" : uniform(0, 0.4)   \n",
    "    },\n",
    "    \"bayes\": {\n",
    "        \"hidden_layer_sizes\" : [\"(10,)\",\"(50,)\",\"(100,)\",\"(10,10)\",\"(50,50)\",\"(60,60)\",\"(100,50)\",\"(100,100)\",\"(100,50,25)\"],\n",
    "        \"activation_name\" :  [\"Relu\", \"Sigmoid\", \"Tanh\", \"Leaky_relu\", \"Softmax\"],\n",
    "        \"learning_rate\" : Real(1e-4, 1e-1, prior='log-uniform'),\n",
    "        \"batch_size\" : list(np.arange(10,500, 10)),\n",
    "        \"optimizer_name\" : [\"Adam\", \"SGD\"],\n",
    "        \"alpha\" : np.logspace(-3,0,19),\n",
    "        \"weight_decay\" : Real(1e-5, 1, prior='log-uniform'),\n",
    "        \"p_dropout\" : Real(0, 0.4, prior='uniform')\n",
    "    }\n",
    "}\n",
    "\n",
    "model_mlp = MLPBinaryClassifier(X=X_labeled, y=y_labeled, X_unlabeled=X_unlabeled, split_test=0.2, **params[\"init\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02818466",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 5\n",
    "n_points=1\n",
    "cv=5\n",
    "scoring='accuracy'\n",
    "verbose=3\n",
    "n_jobs=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5845bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_mlp.bayes_search(\n",
    "#     param_bayes=params['bayes'],\n",
    "#     n_iter=n_iter,\n",
    "#     n_points=n_points,\n",
    "#     cv=cv,\n",
    "#     scoring=scoring,\n",
    "#     n_jobs=n_jobs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63527ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_mlp.randomized_search(\n",
    "#     param_randomized=params['randomized'],\n",
    "#     n_iter=n_iter,\n",
    "#     cv=cv,\n",
    "#     scoring=scoring,\n",
    "#     n_jobs=n_jobs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407024f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp.model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6b9200",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521018d1",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "\n",
    "\n",
    "For a classification problem, few elements must be identified:\n",
    "\n",
    "- **TP** (True Positives) is the number of correctly classified *Charged Off* loans.\n",
    "- **TN** (True Negatives) is the number of correctly classified *Fully Paid* loans.\n",
    "- **FP** (False Positives) is the number of *Fully Paid* loans incorrectly classified as 'Charged Off'.\n",
    "- **FN** (False Negatives) is the number of *Charged Off* loans incorrectly classified as 'Fully Paid'.\n",
    "\n",
    "Here a metrics to evaluate the performance of the model:\n",
    "\n",
    "*Precision, Recall and F1Score provide a better understanding of how well the model is performing in identifying the *Charge Off* loans, the minority target of the dataset.*\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "This metrics is the main metric to evaluate the model through the hyperparameters search.\n",
    "\n",
    "$$ Accuracy = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    "\n",
    "### Precision\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "### Recall - Sensitivity\n",
    "\n",
    "$$ Accuracy = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "### F1Score\n",
    "\n",
    "$$ F1Score = 2 * \\frac{Precision * Recall}{Precision + Recall} $$\n",
    "\n",
    "### ROC-AUC\n",
    "\n",
    "The area under the receiver operation characteristic curve is a single value summarizing the overall ability of the model.\n",
    "\n",
    "### Observations\n",
    "\n",
    "The trained model well performed a binary classification on the dataset since metrics are near from 1, i.e. higher than 0,99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedc829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp.model_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4212cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp.receiver_operating_characteristics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac53482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_spark.sample(fraction=0.05, seed=30)\n",
    "df_test = df_test.dropna(subset=['loan_status'])\n",
    "features_collected = df_test.select(features).collect()\n",
    "X_test = np.array([list(feature) for feature in features_collected])\n",
    "target_collected = df_test.select('loan_status').collect()\n",
    "y_test = np.array([feature['loan_status'] for feature in target_collected])\n",
    "y_test = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09624bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp.model_performance_test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a3b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_value_collected = df_spark.sample(withReplacement=False, fraction=0.0001, seed=1).limit(1).collect()[0]\n",
    "single_value = np.array([value for key, value in single_value_collected.asDict().items() if key != 'loan_status']).reshape(1,-1).astype(float)\n",
    "single_value_target = np.array([value for key, value in single_value_collected.asDict().items() if key == 'loan_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02be47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp.compute_integrated_gradients(single_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea02de2d",
   "metadata": {},
   "source": [
    "# Data Visualization\n",
    "\n",
    "If I have more time, I would like to display the prediction areas of the classifier in multiple 2D or 3D graphics, taking different features in axes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198bfd39",
   "metadata": {},
   "source": [
    "# Saving \n",
    "\n",
    "I saved a trained MLP classifier model with initial configuration for my docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0412b1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('app/lending_club_mlp_binary_classifier.pkl', 'wb') as file:\n",
    "    pickle.dump(model_mlp, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f6e7c",
   "metadata": {},
   "source": [
    "# Application\n",
    "\n",
    "The docker image of the application is located [here](https://hub.docker.com/repository/docker/yanncauchepin/lendingclub/general).\n",
    "\n",
    "I realized that I have not handle the case where one of the feature filled to describe the data is NaN. I would probably code a inner function of the model to process an imputation with a more advanced method than just setting the mean value of the feature.\n",
    "\n",
    "Due to an additionnal line that I forgot to removed in the past, the application and its model do not handle the feature *last_pymnt_d* while it is well selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f616892",
   "metadata": {},
   "source": [
    "# Thank you\n",
    "\n",
    "I would like to thank you for taking the time to read this hands-on! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
